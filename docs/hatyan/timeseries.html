<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>hatyan.timeseries API documentation</title>
<meta name="description" content="timeseries.py contains all definitions related to hatyan timeseries â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hatyan.timeseries</code></h1>
</header>
<section id="section-intro">
<p>timeseries.py contains all definitions related to hatyan timeseries.</p>
<p>hatyan is a Python program for tidal analysis and prediction, based on the FORTRAN version.
Copyright (C) 2019-2021 Rijkswaterstaat.
Maintained by Deltares, contact: Jelmer Veenstra (jelmer.veenstra@deltares.nl).
Source code available at: <a href="https://github.com/Deltares/hatyan">https://github.com/Deltares/hatyan</a></p>
<p>This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.</p>
<p>This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
See the
GNU Lesser General Public License for more details.</p>
<p>You should have received a copy of the GNU Lesser General Public License
along with this program.
If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
timeseries.py contains all definitions related to hatyan timeseries.

hatyan is a Python program for tidal analysis and prediction, based on the FORTRAN version. 
Copyright (C) 2019-2021 Rijkswaterstaat.  Maintained by Deltares, contact: Jelmer Veenstra (jelmer.veenstra@deltares.nl). 
Source code available at: https://github.com/Deltares/hatyan

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

&#34;&#34;&#34;

import os
import io
import numpy as np
import pandas as pd
import datetime as dt
import scipy.signal as ssig
file_path = os.path.realpath(__file__)
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq
from netCDF4 import Dataset, date2num, stringtoarr#, num2date
from hatyan.foreman import get_foreman_v0_freq
from hatyan.schureman import get_schureman_freqs
from hatyan.hatyan_core import get_const_list_hatyan


def calc_HWLW(ts, calc_HWLW345=False, calc_HWLW1122=False, debug=False, buffer_hr=6):
    &#34;&#34;&#34;
    
    Calculates extremes (high and low waters) for the provided timeseries. 
    This definition uses scipy.signal.find_peaks() with arguments &#39;distance&#39; and &#39;prominence&#39;. 
    The minimal &#39;distance&#39; between two high or low water peaks is based on the M2 period: 12.42/1.5=8.28 hours for HW and 12.42/1.7=7.30 hours for LW (larger because of aggers). 
    The prominence for local extremes is set to 0.01m, to filter out very minor dips in the timeseries.
    If there are two equal high or low water values, the first one is taken. 
    There are no main high/low waters calculated within 6 hours of the start/end of the timeseries (keyword buffer_hr), since these can be invalid.
    This function can deal with gaps. Since scipy.signal.find_peaks() warns about nan values, those are removed first.
    This does influence the results since find_peaks does not know about time registration. This is also tricky for input timeseries with varying time interval.
    
    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries with a tidal prediction or water level measurements.
    calc_HWLW345 : boolean, optional
        Whether to also calculate local extremes, first/second low waters and &#39;aggers&#39;. 
        The default is False, in which case only extremes per tidal period are calculated.
        When first/second low waters and aggers are calculated, the local extremes around highwater (eg double highwaters and dips) are filtered out first.
    calc_HWLW345_cleanup1122 : boolean, optional
        Whether to remove HWLWcodes 11 and 22 from DataFrame. The default is True.
    debug : boolean, optional
        Whether to print debug information. The default is False.
    
    Raises
    ------
    Exception
        DESCRIPTION.
    
    Returns
    -------
    data_pd_HWLW : pandas.DataFrame
        The DataFrame contains colums &#39;times&#39;, &#39;values&#39; and &#39;HWLWcode&#39;, it contains the times, values and codes of the timeseries that are extremes.
        1 (high water) and 2 (low water). And if calc_HWLW345=True also 3 (first low water), 4 (agger) and 5 (second low water).

    &#34;&#34;&#34;
    
    if not ts.index.is_monotonic_increasing:
        raise Exception(&#39;ERROR: timeseries is not monotonic increasing, supply sorted timeseries (ts = ts.index.sort_index()&#39;) #otherwise &#34;ValueError: &#39;list&#39; argument must have no negative elements&#34;
    
    #calculate the amount of steps in a M2 period, based on the most occurring timestep 
    M2_period_min = get_schureman_freqs([&#39;M2&#39;]).loc[&#39;M2&#39;,&#39;period [hr]&#39;]*60
    ts_steps_min_most = np.argmax(np.bincount((ts.index.to_series().diff().iloc[1:].dt.total_seconds()/60).astype(int).values))
    if ts_steps_min_most &gt; 1:
        print(&#39;WARNING: the timestep of the series for which to calculate extremes/HWLW is %i minutes, but 1 minute is recommended&#39;%(ts_steps_min_most))
    M2period_numsteps = M2_period_min/ts_steps_min_most
    
    data_pd_HWLW = pd.DataFrame({&#39;times&#39;:ts.index,&#39;values&#39;:ts[&#39;values&#39;],&#39;HWLWcode&#39;:np.nan}).reset_index(drop=True)
    #create empty HWLW dataframe
    if data_pd_HWLW[&#39;values&#39;].isnull().any():
        data_pd_HWLW = data_pd_HWLW[~data_pd_HWLW[&#39;values&#39;].isnull()].reset_index(drop=True)
        print(&#39;WARNING: the provided ts for extreme/HWLW calculation contained NaN values. To avoid unexpected results from scipy.signal.find_peaks(), the %i NaN values were removed from the ts (%.2f%%) before calculating extremes/HWLW.&#39;%(len(ts)-len(data_pd_HWLW), (len(ts)-len(data_pd_HWLW))/len(ts)*100))

    if calc_HWLW345 or calc_HWLW1122:
        #get all local extremes, including aggers and second high waters (1/2/11/22) #takes first value of two equal peaks, prominence naar 0.01 om matige aggers uit te sluiten
        LWid_all, LWid_all_properties = ssig.find_peaks(-data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=None)
        HWid_all, HWid_all_properties = ssig.find_peaks(data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=None)
        data_pd_HWLW.loc[LWid_all,&#39;HWLWcode&#39;] = 22 #all LW
        data_pd_HWLW.loc[HWid_all,&#39;HWLWcode&#39;] = 11 #all HW

    #get HWLW (extremes per tidal period). 
    LWid_main_raw,LWid_main_properties = ssig.find_peaks(-data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=M2period_numsteps/1.7) #most stations work with factor 1.4. 1.5 results in all LW values for HoekvanHolland for 2000, 1.7 results in all LW values for Rotterdam for 2000 (also for 1999-2002).
    HWid_main_raw,HWid_main_properties = ssig.find_peaks(data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=M2period_numsteps/1.9) #most stations work with factor 1.4. 1.5 value results in all HW values for DenHelder for year 2000 (also for 1999-2002). 1.7 results in all HW values for LITHDP 2018. 1.9 results in all correct values for LITHDP 2022
    # remove main extremes within 6 hours of start/end of timeseries, since they are often missed or invalid.
    validtimes_idx = data_pd_HWLW.loc[(data_pd_HWLW[&#39;times&#39;]&gt;=ts.index[0]+dt.timedelta(hours=buffer_hr)) &amp; (data_pd_HWLW[&#39;times&#39;]&lt;=ts.index[-1]-dt.timedelta(hours=buffer_hr))].index
    LWid_main = LWid_main_raw[np.in1d(LWid_main_raw,validtimes_idx)]
    HWid_main = HWid_main_raw[np.in1d(HWid_main_raw,validtimes_idx)]
    #use valid values to continue process
    data_pd_HWLW.loc[LWid_main,&#39;HWLWcode&#39;] = 2
    data_pd_HWLW.loc[HWid_main,&#39;HWLWcode&#39;] = 1
    
    #drop all non-(local)extreme timesteps and convert HWLWcode column to integers
    data_pd_HWLW = data_pd_HWLW.dropna(subset=[&#39;HWLWcode&#39;])
    data_pd_HWLW[&#39;HWLWcode&#39;] = data_pd_HWLW[&#39;HWLWcode&#39;].astype(int)

    if debug: #debug statistics
        prop_list = [&#39;prominences&#39;,&#39;widths&#39;]
        data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==2,prop_list] = pd.DataFrame(LWid_main_properties,index=LWid_main_raw).loc[LWid_main,prop_list]
        print(&#39;LW values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==2]))
        data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==1,prop_list] = pd.DataFrame(HWid_main_properties,index=HWid_main_raw).loc[HWid_main,prop_list]
        print(&#39;HW values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==1]))
        if calc_HWLW345 or calc_HWLW1122:
            LW_local_bool = ~np.in1d(LWid_all, LWid_main)
            data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==22,prop_list] = pd.DataFrame(LWid_all_properties,index=LWid_all).loc[LW_local_bool,prop_list]
            print(&#39;LW_local values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==22]))
            HW_local_bool = ~np.in1d(HWid_all, HWid_main)
            data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==11,prop_list] = pd.DataFrame(HWid_all_properties,index=HWid_all).loc[HW_local_bool,prop_list]
            print(&#39;HW_local values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==11]))
    
    if calc_HWLW345: #recalculate local LW/HWs between two main HWs to firstLW/agger/secondLW
        data_pd_HWLW = calc_HWLWlocalto345(data_pd_HWLW,HWid_main)
    
    #return to normal time-index
    data_pd_HWLW = data_pd_HWLW.set_index(&#39;times&#39;)
    return data_pd_HWLW


def calc_HWLWlocalto345(data_pd_HWLW,HWid_main):
    &#34;&#34;&#34;
    Recalculate local LW/HWs between two main HWs to firstLW/agger/secondLW

    Parameters
    ----------
    data_pd_HWLW : TYPE
        DESCRIPTION.
    HWid_main : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    data_pd_HWLW : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    
    print(&#39;calculating 1stLW/agger/2ndLW for all tidalperiods (between two HW values)...&#39;)
    for iTide, dummy in enumerate(HWid_main[:-1]):
        data_pd_HWLW_1tide = data_pd_HWLW.loc[HWid_main[iTide]:HWid_main[iTide+1],:]
        
        #filter local extremes around HW (only interested in aggers, so LW), this is necessary for eg DENHDR and PETTZD, otherwise second HW is seen as first LW
        data_pd_HWLW_1tide_minHW = data_pd_HWLW_1tide.loc[data_pd_HWLW_1tide[&#39;HWLWcode&#39;]==1,[&#39;values&#39;]].min()[0]
        data_pd_HWLW_1tide_min = data_pd_HWLW_1tide[&#39;values&#39;].min()
        data_pd_HWLW_1tide_mid = np.mean([data_pd_HWLW_1tide_minHW,data_pd_HWLW_1tide_min])
        bool_LWs = data_pd_HWLW_1tide[&#39;values&#39;]&lt;data_pd_HWLW_1tide_mid
        data_pd_HWLW_1tide_noHWs = data_pd_HWLW_1tide[bool_LWs]
        
        if len(data_pd_HWLW_1tide_noHWs) &gt; 3: #(attempt to) reduce to three values between two HWs
            print(&#39;WARNING: more than 3 values between HWs, removing part of them&#39;)
            #print(data_pd_HWLW_1tide)
            agger35_prim = data_pd_HWLW_1tide_noHWs[data_pd_HWLW_1tide_noHWs[&#39;HWLWcode&#39;]==2]
            if len(agger35_prim)&gt;1:
                raise Exception(&#39;should be only one HWLWcode=2 per tide period&#39;)
            agger35_prim_loc = agger35_prim.index[0]
            agger35_sec_loc = data_pd_HWLW_1tide_noHWs.loc[data_pd_HWLW_1tide_noHWs[&#39;HWLWcode&#39;]==22,&#39;values&#39;].idxmin()
            agger35_loc = np.sort([agger35_prim_loc,agger35_sec_loc])
            data_pd_HWLW_1tide_noHWs = data_pd_HWLW_1tide_noHWs.loc[agger35_loc.min():agger35_loc.max(),:]
            agger4_loc = data_pd_HWLW_1tide_noHWs[&#39;values&#39;].idxmax()
            data_pd_HWLW_1tide_noHWs = data_pd_HWLW_1tide_noHWs.loc[[agger35_loc.min(),agger4_loc,agger35_loc.max()],:]
        
        if len(data_pd_HWLW_1tide_noHWs) == 1: #primary low water already has code 2
            if data_pd_HWLW_1tide_noHWs[&#39;HWLWcode&#39;].iloc[0] != 2:
                raise Exception(&#39;Only 1 LW value but does not have HWLWcode 2&#39;)
        elif len(data_pd_HWLW_1tide_noHWs) == 3:
            if not data_pd_HWLW_1tide_noHWs[&#39;values&#39;].argmax() == 1:
                raise Exception(&#39;3 values between two HW values, but center one is not the largest:\n%s&#39;%(data_pd_HWLW_1tide))
            agger345_loc = data_pd_HWLW_1tide_noHWs.index
            if not (data_pd_HWLW.loc[agger345_loc[0],&#39;HWLWcode&#39;] in [2,22] and data_pd_HWLW.loc[agger345_loc[1],&#39;HWLWcode&#39;] in [11] and data_pd_HWLW.loc[agger345_loc[2],&#39;HWLWcode&#39;] in [2,22]):
                raise Exception(&#39;3 values between two HW values, but do not correspond to LW/agger/LW:\n%s&#39;%(data_pd_HWLW_1tide))
            data_pd_HWLW.loc[agger345_loc,&#39;HWLWcode&#39;] = [3,4,5]
        elif len(data_pd_HWLW_1tide_noHWs) == 2:
            print(&#39;WARNING: 2 values left between two HWs (slightly unexpected):\n%s&#39;%(data_pd_HWLW_1tide))
        else:
            raise Exception(&#39;unexpected number of values between two HWs (0 or more than 3):\n%s&#39;%(data_pd_HWLW_1tide))
    
    #remove remaining 11 and 22 values from array
    #if calc_HWLW345_cleanup1122:
    data_pd_HWLW = data_pd_HWLW.drop(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==11].index)
    data_pd_HWLW = data_pd_HWLW.drop(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==22].index)
    
    print(&#39;finished calculating 1stLW/agger/2ndLW for all tidalperiods&#39;)
    
    return data_pd_HWLW


def calc_HWLW12345to21(data_HWLW_12345): #TODO: if first/last timestep is LW, these are not returned (loops from HW to HW)
    &#34;&#34;&#34;
    

    Parameters
    ----------
    data_HWLW12345 : TYPE
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    print(&#39;starting HWLW 12345 to 12 correction&#39;)
    times_LWmin = []
    data_HW1 = data_HWLW_12345.loc[data_HWLW_12345[&#39;HWLWcode&#39;]==1]
    for iHW in np.arange(0,len(data_HW1)-1):
        tide_afterHW = data_HWLW_12345.loc[data_HW1.index[iHW]:data_HW1.index[iHW+1]]
        time_minimum = tide_afterHW[&#39;values&#39;].idxmin()
        times_LWmin.append(time_minimum)
    data_LW2 = data_HWLW_12345.loc[times_LWmin]
    data_LW2[&#39;HWLWcode&#39;] = 2
    data_HWLW_12 = pd.concat([data_HW1,data_LW2]).sort_index()
    
    return data_HWLW_12


def calc_HWLWnumbering(ts_ext, station=None, corr_tideperiods=None):
    &#34;&#34;&#34;
    For calculation of the extremes numbering, w.r.t. the first high water at Cadzand in 2000 (occurred on 1-1-2000 at approximately 9:45). 
    The number of every high and low water is calculated by taking the time difference between itself and the first high water at Cadzand, correcting it with the station phase difference (M2phasediff). 
    Low waters are searched for half an M2 period from the high waters. 
    By adding a search window of half the period of M2 (searchwindow_hr), even strong time variance between consecutive high or low waters should be caputered. 
    
    Parameters
    ----------
    ts_ext : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLWcode&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.
    station: string, optional
        The station for which the M2 phase difference should be retrieved from data_M2phasediff_perstation.txt.
        This value is the phase difference in degrees of the occurrence of the high water generated by the same tidal wave as the first high water in 2000 at Cadzand (actually difference between M2 phases of stations).
        This value is used to correct the search window of high/low water numbering. The default is None.
    corr_tideperiods : integer, optional
        Test keyword to derive HWLWnumbering with a n*360 degrees offset only, but this does not work properly. The default is None.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    ts_ext : pandas.DataFrame
        The input DataFrame with the column &#39;HWLWno&#39; added, which contains the numbers of the extremes.

    &#34;&#34;&#34;
        
    M2_period_hr = get_schureman_freqs([&#39;M2&#39;]).loc[&#39;M2&#39;,&#39;period [hr]&#39;]
    firstHWcadz_fixed = dt.datetime(2000, 1, 1, 9, 45)
    searchwindow_hr = M2_period_hr/2
    
    if not all((ts_ext[&#39;HWLWcode&#39;]==1) | (ts_ext[&#39;HWLWcode&#39;]==2) | (ts_ext[&#39;HWLWcode&#39;]==3) | (ts_ext[&#39;HWLWcode&#39;]==4) | (ts_ext[&#39;HWLWcode&#39;]==5)):
        raise Exception(&#39;calc_HWLWnumbering() not implemented for HWLWcode other than 1,2,3,4,5 (so no HWLWcode 11 or 22 supported), provide extreme timeseries derived with Timeseries.calc_HWLW(calc_HWLW345=False) or Timeseries.calc_HWLW(calc_HWLW345=True, calc_HWLW345_cleanup1122=True)&#39;)
    ts_ext = ts_ext.copy()
    
    HW_bool = ts_ext[&#39;HWLWcode&#39;]==1
    HW_tdiff_cadzdraw = (ts_ext.loc[HW_bool].index.to_series()-firstHWcadz_fixed).dt.total_seconds()/3600
    if station is None:
        HW_tdiff_cadzdraw_M2remainders = (HW_tdiff_cadzdraw)%M2_period_hr
        M2phasediff_hr = (HW_tdiff_cadzdraw_M2remainders).mean()
        M2phasediff_deg = M2phasediff_hr/M2_period_hr*360
        print(&#39;no value or None for argument M2phasediff provided, automatically calculated correction w.r.t. Cadzand is %.2f hours (%.2f degrees)&#39;%(M2phasediff_hr, M2phasediff_deg))
        if corr_tideperiods is not None:
            M2phasediff_deg = M2phasediff_deg+corr_tideperiods
            M2phasediff_hr = M2phasediff_deg/360*M2_period_hr
            print(&#39;additional tideperiod correction provided via corr_tideperiods of %.1f degrees, new correction w.r.t. Cadzand is %.2f hours (%.2f degrees)&#39;%(corr_tideperiods, M2phasediff_hr, M2phasediff_deg))
    else:
        file_M2phasediff = os.path.join(os.path.dirname(file_path),&#39;data&#39;,&#39;data_M2phasediff_perstation.txt&#39;)
        stations_M2phasediff = pd.read_csv(file_M2phasediff, names=[&#39;M2phasediff&#39;], comment=&#39;#&#39;, delim_whitespace=True)
        if station not in stations_M2phasediff.index:
            raise Exception(f&#39;ERROR: station &#34;{station}&#34; not in file_M2phasediff ({file_M2phasediff})&#39;)
        stat_M2phasediff = stations_M2phasediff.loc[station,&#39;M2phasediff&#39;]
        M2phasediff_hr = stat_M2phasediff/360*M2_period_hr
    HW_tdiff_cadzd = HW_tdiff_cadzdraw - M2phasediff_hr + searchwindow_hr
    HW_tdiff_div, HW_tdiff_mod_searchwindow = np.divmod(HW_tdiff_cadzd.values, M2_period_hr)
    HW_tdiff_mod = HW_tdiff_mod_searchwindow - searchwindow_hr
    if not all(np.diff(HW_tdiff_div) &gt; 0):
        raise Exception(&#39;tidal wave numbering: HW numbers not always increasing&#39;)
    if not all(np.abs(HW_tdiff_mod)&lt;searchwindow_hr):
        raise Exception(&#39;tidal wave numbering: not all HW fall into hardcoded search window&#39;)
    ts_ext.loc[HW_bool,&#39;HWLWno&#39;] = HW_tdiff_div
    
    for LWcode_2345 in [2,3,4,5]:
        LW_bool = ts_ext[&#39;HWLWcode&#39;]==LWcode_2345
        LW_tdiff_cadzdraw = (ts_ext.loc[LW_bool].index.to_series()-firstHWcadz_fixed).dt.total_seconds()/3600
        LW_tdiff_cadzd = LW_tdiff_cadzdraw - M2phasediff_hr + searchwindow_hr - M2_period_hr/2
        LW_tdiff_div, LW_tdiff_mod_searchwindow = np.divmod(LW_tdiff_cadzd.values, M2_period_hr)
        LW_tdiff_mod = LW_tdiff_mod_searchwindow - searchwindow_hr
        if not all(np.diff(LW_tdiff_div) &gt; 0):
            raise Exception(&#39;tidal wave numbering: LW numbers not always increasing&#39;)
        if not all(np.abs(LW_tdiff_mod)&lt;searchwindow_hr):
            raise Exception(&#39;tidal wave numbering: not all LW fall into defined search window&#39;)
        ts_ext.loc[LW_bool,&#39;HWLWno&#39;] = LW_tdiff_div
    
    #check if LW is after HW
    ts_ext_checkfirst = ts_ext[ts_ext[&#39;HWLWno&#39;]==np.min(HW_tdiff_div)]
    tdiff_firstHWLW = (ts_ext_checkfirst.index.to_series().diff().dt.total_seconds()/3600).values[1]
    if (tdiff_firstHWLW&lt;0) or (tdiff_firstHWLW&gt;M2_period_hr):
        raise Exception(&#39;tidal wave numbering: first LW does not match first HW&#39;)
    
    ts_ext[&#39;HWLWno&#39;] = ts_ext[&#39;HWLWno&#39;].astype(int)
    
    return ts_ext


def timeseries_fft(ts_residue, min_prominence=10**3, max_freqdiff=None, plot_fft=True, source=&#39;schureman&#39;):
    
    print(&#39;analyzing timeseries with fft and fftfreq&#39;)
    
    if ts_residue[&#39;values&#39;].isnull().sum() &gt; 0:
        raise Exception(&#39;supplied timeseries contains nan values, use pd.interpolate first (dropping them will result in non-constant timestep which is also not possible for fft)&#39;)
    y = ts_residue[&#39;values&#39;].values
    N = len(y)
    T = np.unique((ts_residue.index[1:]-ts_residue.index[:-1])).astype(float)/1e9/3600 #timestep in hours.
    if len(T)!=1:
        raise Exception(&#39;timestep of supplied timeseries should be constant for fourier analysis&#39;)
    yf = fft(y)
    power = np.abs(yf)
    freq = fftfreq(N, T[0])
    peaks, peaks_properties = ssig.find_peaks(power[freq &gt;=0], prominence=min_prominence)
    peak_freq =  freq[peaks]
    peak_power = power[peaks]
    
    if plot_fft:
        fig,ax = plt.subplots()
        ax.plot(freq[:N//2], power[:N//2])
        ax.plot(peak_freq, peak_power, &#39;ro&#39;)
        ax.grid()
        ax.set_xlim(0,0.5)
    
    if source==&#39;schureman&#39;:
        const_list_all = get_const_list_hatyan(listtype=&#39;all_schureman&#39;)
        hatyan_freqs = get_schureman_freqs(const_list=const_list_all)
    elif source==&#39;foreman&#39;:
        const_list_all = get_const_list_hatyan(listtype=&#39;all_foreman&#39;)
        dummy, hatyan_freqs = get_foreman_v0_freq(const_list=const_list_all)
        hatyan_freqs[&#39;period [hr]&#39;] = 1/hatyan_freqs[&#39;freq&#39;]
    
    const_closest = []
    for peak_freq_one in peak_freq:
        hatyan_freqs_closest = hatyan_freqs.iloc[np.argmin(np.abs(hatyan_freqs[&#39;freq&#39;]-peak_freq_one)),:] #TODO: freq is not always close enough but is still added to list
        const_closest.append(hatyan_freqs_closest.name)
    hatyan_freqs_suggestions = hatyan_freqs.loc[const_closest,[&#39;freq&#39;,&#39;period [hr]&#39;]]
    hatyan_freqs_suggestions[&#39;peak_freq&#39;] = peak_freq
    hatyan_freqs_suggestions[&#39;peak_freqdiff&#39;] = (hatyan_freqs_suggestions[&#39;freq&#39;] - hatyan_freqs_suggestions[&#39;peak_freq&#39;]).abs()
    hatyan_freqs_suggestions[&#39;peak_prominences&#39;] = peaks_properties[&#39;prominences&#39;]
    if max_freqdiff is not None:
        #select below freqdiff treshold
        hatyan_freqs_suggestions = hatyan_freqs_suggestions.loc[hatyan_freqs_suggestions[&#39;peak_freqdiff&#39;]&lt;max_freqdiff]
    print(&#39;suggested constituents+freqs from hatyan:\n%s&#39;%(hatyan_freqs_suggestions))
    
    return hatyan_freqs_suggestions


def plot_timeseries(ts, ts_validation=None, ts_ext=None, ts_ext_validation=None):
    &#34;&#34;&#34;
    Creates a plot with the provided timeseries

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    ts_validation : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries. The default is None.
    ts_ext : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.
    ts_ext_validation : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.

    Returns
    -------
    fig : matplotlib.figure.Figure
        The generated figure handle, with which the figure can be adapted and saved.
    axs : (tuple of) matplotlib.axes._subplots.AxesSubplot
        The generated axis handle, whith which the figure can be adapted.

    &#34;&#34;&#34;
        
    size_figure = (15,9)
    size_line_ts = 0.7
    size_marker_ts = 1
    figure_ylim_ts = [-3,3]
    figure_ylim_tsdiff = [-0.02,0.02]
    
    if ts_validation is not None:
        times_predval_ext = [min(min(ts_validation.index),min(ts.index)), max(max(ts_validation.index),max(ts.index))]

    else:
        times_predval_ext = [min(ts.index), max(ts.index)]    

    fig, (ax1, ax2) = plt.subplots(2,1,figsize=size_figure, sharex=True, gridspec_kw={&#39;height_ratios&#39;:[2,1]})
    
    ax1.set_title(&#39;hatyan timeseries&#39;)
    ax1.plot(ts.index, ts[&#39;values&#39;],&#39;o-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;ts&#39;)
    if ts_validation is not None:
        #overlap between timeseries for difference plots
        times_id_validationinpred = np.where(ts_validation.index.isin(ts.index))[0]
        times_id_predinvalidation = np.where(ts.index.isin(ts_validation.index))[0]
        ax1.plot(ts_validation.index, ts_validation[&#39;values&#39;],&#39;o-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;ts_validation&#39;, alpha=0.7)
        ax1.plot(ts.index[times_id_predinvalidation], ts[&#39;values&#39;].iloc[times_id_predinvalidation].values-ts_validation[&#39;values&#39;].iloc[times_id_validationinpred].values,&#39;go-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;difference&#39;, alpha=0.7)
    ax1.plot(times_predval_ext,[0,0],&#39;-k&#39;,linewidth=size_line_ts)
    ts_mean = np.mean(ts[&#39;values&#39;])
    ax1.plot(ts.index[[0,-1]],[ts_mean,ts_mean],&#39;-r&#39;,linewidth=size_line_ts,label=&#39;mean of ts&#39;)
    if ts_ext is not None:
        HWLW_codesnames = {1:&#39;HW (1)&#39;,
                           2:&#39;LW (2)&#39;,
                           3:&#39;LW1 (3)&#39;,
                           4:&#39;topagger (4)&#39;,
                           5:&#39;LW2 (5)&#39;,
                           11:&#39;HW_local (11)&#39;,
                           22:&#39;LW_local (22)&#39;}
        for HWLW_code in HWLW_codesnames.keys():
            iExt = ts_ext[&#39;HWLWcode&#39;]==HWLW_code
            if iExt.any():
                HWLW_name = HWLW_codesnames[HWLW_code]
                HWLW_markersize=10
                if HWLW_code in [4,11,22]:
                    HWLW_markersize=5
                ax1.plot(ts_ext.index[iExt],ts_ext[&#39;values&#39;][iExt],&#39;x&#39;,markersize=HWLW_markersize,label=HWLW_name)
    if ts_ext_validation is not None:
        vali_codes = [1,2,3,4,5]
        vali_codenames = [&#39;vali_HW&#39;,&#39;vali_LW&#39;,&#39;vali_LW1&#39;,&#39;vali_topagger&#39;,&#39;vali_LW2&#39;]
        for vali_code, vali_codename in zip(vali_codes,vali_codenames):
            vali_code_ids = ts_ext_validation[&#39;HWLWcode&#39;].values==vali_code
            if any(vali_code_ids): #only plot vali_code in legend if present in HWLW_timeseries
                ax1.plot(ts_ext_validation.index[vali_code_ids],ts_ext_validation[&#39;values&#39;][vali_code_ids],&#39;1&#39;,markersize=10,label=vali_codename)
        #print HWLW statistics
        try:
            plot_HWLW_validatestats(ts_ext=ts_ext, ts_ext_validation=ts_ext_validation, create_plot=False)        
        except:
            print(&#39;WARNING: plot_HWLW_validatestats() failed, probably due to missing HWLWno where autocalculation failed. Consider adding HWLWno to ts_ext and ts_ext_validation with calc_HWLWnumbering() before plotting.&#39;)
    ax1.set_ylim(figure_ylim_ts)
    ax2.set_xlabel(&#39;Time&#39;)
    ax1.set_ylabel(&#39;waterlevel [m]&#39;)
    ax1.legend(loc=&#39;lower right&#39;)
    ax1.grid()
    if ts_validation is not None:
        ax2.plot(ts.index[times_id_predinvalidation], ts[&#39;values&#39;].iloc[times_id_predinvalidation].values-ts_validation[&#39;values&#39;].iloc[times_id_validationinpred].values,&#39;go-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;difference&#39;)
    ax2.plot(times_predval_ext,[0,0],&#39;-k&#39;,linewidth=size_line_ts)
    ax2.set_ylim(figure_ylim_tsdiff)
    rmse = np.nan
    if ts_validation is not None:
        overlapdiff = ts[&#39;values&#39;].iloc[times_id_predinvalidation].values-ts_validation[&#39;values&#39;].iloc[times_id_validationinpred].values
        if len(overlapdiff) != 0:
            rmse = np.sqrt(np.nanmean(overlapdiff ** 2))
    ax2.set_ylabel(&#39;timeseries difference [m], RMSE = %.5f&#39;%(rmse))
    ax2.legend(loc=&#39;lower right&#39;)
    ax2.grid()
    fig.tight_layout()
    
    axs = (ax1,ax2)
    return fig, axs


def plot_HWLW_validatestats(ts_ext, ts_ext_validation, create_plot=True):
    &#34;&#34;&#34;
    This definition calculates (and plots and prints) some statistics when comparing extreme values.
    This is done by calculating the extreme number (sort of relative to Cadzand 1jan2000, but see &#39;warning&#39;) and subtracting the ts_ext and ts_ext_validation dataframes based on these numbers (and HWLWcode).
    It will only result in values for the overlapping extremes, other values will be NaN and are not considered for the statistics.
    Warning: the calculated extreme numbers in this definition are not corrected for the real phase difference with the M2phasediff argument, the calculated extreme are fine for internal use (to match corresponding extremes) but the absolute number might be incorrect.

    Parameters
    ----------
    ts_ext : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.
    ts_ext_validation : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, values and codes of the timeseries that are extremes.
    create_plot : boolean, optional
        Whether to plot the time/value differences or only print the statistics. The default is True.

    Returns
    -------
    fig : matplotlib.figure.Figure
        The generated figure handle, with which the figure can be adapted and saved.
    axs : (tuple of) matplotlib.axes._subplots.AxesSubplot
        The generated axis handle, whith which the figure can be adapted.

    &#34;&#34;&#34;
    
    print(&#39;Calculating comparison statistics for extremes&#39;)
    if &#39;HWLWno&#39; not in ts_ext.columns or &#39;HWLWno&#39; not in ts_ext_validation.columns:
        print(&#39;HWLWno is not present in ts_ext or ts_ext_validation, trying to automatically derive it without M2phasediff argument (this might fail)&#39;)
        try:
            ts_ext_nrs = calc_HWLWnumbering(ts_ext=ts_ext)
            ts_ext_validation_nrs = calc_HWLWnumbering(ts_ext=ts_ext_validation)
        except:
            raise Exception(&#39;ERROR: deriving HWLWno failed, so HWLW statistics cannot be calculated. Add HWLWno with calc_HWLWnumbering() before calling plot_HWLW_validatestats().&#39;)
    else:
        ts_ext_nrs = ts_ext.copy()
        ts_ext_validation_nrs = ts_ext_validation.copy()
    
    #set HWLWcode and HWLWno as index, to make easy subtraction possible
    ts_ext_nrs[&#39;times&#39;] = ts_ext_nrs.index
    ts_ext_nrs = ts_ext_nrs.set_index([&#39;HWLWcode&#39;,&#39;HWLWno&#39;],drop=False)
    ts_ext_validation_nrs[&#39;times&#39;] = ts_ext_validation_nrs.index
    ts_ext_validation_nrs = ts_ext_validation_nrs.set_index([&#39;HWLWcode&#39;,&#39;HWLWno&#39;],drop=False)
    HWLW_diff = ts_ext_nrs.sub(ts_ext_validation_nrs)
    
    tdiff_minutes = HWLW_diff[&#39;times&#39;].dt.total_seconds()/60
    vdiff_cm = HWLW_diff[&#39;values&#39;]*100
    print(&#39;Time differences [minutes]&#39;)
    print(&#39;    RMSE: %.2f&#39;%(np.sqrt(np.mean(tdiff_minutes**2))))
    print(&#39;    std: %.2f&#39;%(tdiff_minutes.std()))
    print(&#39;    abs max: %.2f&#39;%(tdiff_minutes.abs().max()))
    print(&#39;    abs mean: %.2f&#39;%(tdiff_minutes.abs().mean()))
    print(&#39;    #NaN: %i of %i&#39;%(tdiff_minutes.isnull().sum(),len(vdiff_cm)))
    print(&#39;Value differences [cm]&#39;)
    print(&#39;    RMSE: %.2f&#39;%(np.sqrt(np.mean(vdiff_cm**2))))
    print(&#39;    std: %.2f&#39;%(vdiff_cm.std()))
    print(&#39;    abs max: %.2f&#39;%(vdiff_cm.abs().max()))
    print(&#39;    abs mean: %.2f&#39;%(vdiff_cm.abs().mean()))
    print(&#39;    #NaN: %i of %i&#39;%(vdiff_cm.isnull().sum(),len(vdiff_cm)))
    
    if create_plot:
        fig, ax1 = plt.subplots()
        ax1.plot(HWLW_diff.loc[1,&#39;times&#39;].dt.total_seconds()/60,HWLW_diff.loc[1,&#39;values&#39;]*100,&#39;+&#39;,label=&#39;HWdiff&#39;)
        ax1.plot(HWLW_diff.loc[2,&#39;times&#39;].dt.total_seconds()/60,HWLW_diff.loc[2,&#39;values&#39;]*100,&#39;.&#39;,label=&#39;LWdiff&#39;)
        ax1.set_xlabel(&#39;Time difference [minutes]&#39;)
        ax1.set_ylabel(&#39;Value difference [cm]&#39;)
        ax1.legend(loc=1)
        ax1.grid()
    
        axs = (ax1)
        return fig, axs


def write_tsnetcdf(ts, station, vertref, filename, ts_ext=None, tzone_hr=1, nosidx=False, mode=&#39;w&#39;):
    &#34;&#34;&#34;
    Writes the timeseries to a netCDF file

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    station : str
        DESCRIPTION.
    vertref : str
        DESCRIPTION.
    filename : str
        The filename of the netCDF file that will be written.
    ts_ext : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.
    tzone_hr : int, optional
        The timezone (GMT+tzone_hr) that applies to the data. The default is 1 (MET).
    
    Returns
    -------
    None.
    
    &#34;&#34;&#34;
    
    import hatyan
    version_no = hatyan.__version__
    
    times_all = ts.index
    timeseries = ts[&#39;values&#39;]
    times_stepmin = (ts.index[1]-ts.index[0]).total_seconds()/60
    dt_analysistime = dt.datetime.now()
    data_nc = Dataset(filename, mode, format=&#34;NETCDF3_CLASSIC&#34;)
    attr_dict = {&#39;title&#39;: &#39;tidal prediction for %s to %s&#39;%(times_all[0].strftime(&#39;%Y-%m-%d %H:%M:%S&#39;), times_all[-1].strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)),
                 &#39;institution&#39;: &#39;Rijkswaterstaat&#39;,
                 &#39;source&#39;: &#39;hatyan-%s tidal analysis program of Rijkswaterstaat&#39;%(version_no),
                 &#39;timestep_min&#39;: times_stepmin}
    data_nc.setncatts(attr_dict)
    
    ncvarlist = list(data_nc.variables.keys())
    ncdimlist = list(data_nc.dimensions.keys())
    statname_len = 64
    
    if &#39;stations&#39; not in ncdimlist:
        data_nc.createDimension(&#39;stations&#39;,None)
    if &#39;statname_len&#39; not in ncdimlist:
        data_nc.createDimension(&#39;statname_len&#39;,statname_len)
    if &#39;time&#39; not in ncdimlist:
        data_nc.createDimension(&#39;time&#39;,len(times_all.tolist()))
    if &#39;analysis_time&#39; not in ncdimlist:
        data_nc.createDimension(&#39;analysis_time&#39;,1)
    
    refdate_tz = dt.datetime(1900,1,1,tzinfo=dt.timezone(dt.timedelta(hours=tzone_hr)))
    dict_statattr = {&#39;cf_role&#39;: &#39;timeseries_id&#39;}
    dict_anatimattr = {&#39;units&#39;: &#39;minutes since %s&#39;%(refdate_tz.strftime(&#39;%Y-%m-%d %H:%M:%S %z&#39;)), &#39;standard_name&#39;:&#39;forecast_reference_time&#39;, &#39;long_name&#39;:&#39;forecast_reference_time&#39;}
    dict_timattr = {&#39;units&#39;: &#39;minutes since %s&#39;%(refdate_tz.strftime(&#39;%Y-%m-%d %H:%M:%S %z&#39;))}
    dict_wlattr = {&#39;units&#39;:&#39;m&#39;, &#39;vertical_reference&#39;: vertref, &#39;standard_name&#39;: &#39;sea_surface_height_above_geopotential_datum&#39;, &#39;long_name&#39;: &#39;astronomical prediction of water level above reference level&#39;}
    dict_HWattr = {&#39;units&#39;:&#39;m&#39;, &#39;vertical_reference&#39;: vertref, &#39;standard_name&#39;: &#39;sea_surface_height_above_geopotential_datum&#39;, &#39;long_name&#39;: &#39;astronomical prediction of high water extremes above reference level&#39;}
    dict_LWattr = {&#39;units&#39;:&#39;m&#39;, &#39;vertical_reference&#39;: vertref, &#39;standard_name&#39;: &#39;sea_surface_height_above_geopotential_datum&#39;, &#39;long_name&#39;: &#39;astronomical prediction of low water extremes above reference level&#39;}
    dict_HWLWnoattr = {&#39;units&#39;:&#39;n-th tidal wave since reference wave at Cadzand on 1-1-2000&#39;} #, &#39;standard_name&#39;: &#39;&#39;, &#39;long_name&#39;: &#39;&#39;}

    if &#39;stations&#39; not in ncvarlist: #create empty variables if not yet present
        nc_newvar = data_nc.createVariable(&#39;stations&#39;,&#39;S1&#39;,(&#39;stations&#39;,&#39;statname_len&#39;,))
        nc_newvar.setncatts(dict_statattr)
    
    if &#39;analysis_time&#39; not in ncvarlist:
        nc_newvar = data_nc.createVariable(&#39;analysis_time&#39;,&#39;f8&#39;,(&#39;analysis_time&#39;,))
        nc_newvar.setncatts(dict_anatimattr)
        data_nc.variables[&#39;analysis_time&#39;][0] = date2num([dt_analysistime], units=data_nc.variables[&#39;analysis_time&#39;].units)   
    
    #current length is used as index
    nstat = data_nc.variables[&#39;stations&#39;].shape[0]
    #append current data to netcdf files
    data_nc.variables[&#39;stations&#39;][nstat,:] = stringtoarr(station, statname_len, dtype=&#39;S&#39;)
    
    #general prediction
    if &#39;time&#39; not in ncvarlist:
        nc_newvar = data_nc.createVariable(&#39;time&#39;,&#39;f8&#39;,(&#39;time&#39;,))
        nc_newvar.setncatts(dict_timattr)
        #set time contents upon creation of variable, is constant over loop
        data_nc.variables[&#39;time&#39;][:] = date2num(times_all.tolist(),units=data_nc.variables[&#39;time&#39;].units)
    if &#39;waterlevel_astro&#39; not in ncvarlist:
        nc_newvar = data_nc.createVariable(&#39;waterlevel_astro&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;time&#39;,))
        nc_newvar.setncatts(dict_wlattr)
    data_nc.variables[&#39;waterlevel_astro&#39;][nstat,:] = timeseries
    
    if ts_ext is None:
        print(&#39;no HWLW prediction written&#39;)
        data_nc.close()
        return #this skips the HWLW part of the definition
    
    #HWLW prediction
    if nosidx:
        #convert index from time to HWLWno
        data_HWLW_nosidx = ts_ext.copy()
        data_HWLW_nosidx[&#39;times&#39;] = data_HWLW_nosidx.index
        data_HWLW_nosidx = data_HWLW_nosidx.set_index(&#39;HWLWno&#39;)
        #data_HWLW_nosidx = data_HWLW_nosidx.sort_index()
        HWLWno_all = data_HWLW_nosidx.index.unique()
        #data_HW = pd.DataFrame(data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==1],index=HWLWno_all)
        #data_LW = pd.DataFrame(data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==2],index=HWLWno_all)
        data_HW = data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==1]
        data_LW = data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==2]
        bool_HW = HWLWno_all.isin(data_HW.index)
        bool_LW = HWLWno_all.isin(data_LW.index)
        
        #HWLWno
        if &#39;HWLWno&#39; not in ncdimlist:
            data_nc.createDimension(&#39;HWLWno&#39;,len(HWLWno_all))
        if &#39;HWLWno&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;HWLWno&#39;,&#39;i&#39;,(&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_HWLWnoattr)
        data_nc.variables[&#39;HWLWno&#39;][:] = HWLWno_all
        #HW
        if &#39;times_astro_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;times_astro_HW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;times_astro_HW&#39;][nstat,bool_HW] = date2num(data_HW[&#39;times&#39;].tolist(),units=data_nc.variables[&#39;times_astro_HW&#39;].units)
        if &#39;waterlevel_astro_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_HW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_HWattr)
        data_nc.variables[&#39;waterlevel_astro_HW&#39;][nstat,bool_HW] = data_HW[&#39;values&#39;]
        #LW
        if &#39;times_astro_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;times_astro_LW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,)) 
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;times_astro_LW&#39;][nstat,bool_LW] = date2num(data_LW[&#39;times&#39;].tolist(),units=data_nc.variables[&#39;times_astro_LW&#39;].units)
        if &#39;waterlevel_astro_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_LW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_LWattr)
        data_nc.variables[&#39;waterlevel_astro_LW&#39;][nstat,bool_LW] = data_LW[&#39;values&#39;]
    
    else: #use time as index and create array with gaps (not possible to combine multiple stations)
        if nstat&gt;0:
            raise Exception(f&#39;with nosidx={nosidx} it is not possible to write multiple stations per file&#39;)
        data_HWLW = ts_ext.copy()
        data_HWLW = data_HWLW.sort_index(axis=0)
        data_HW = data_HWLW[data_HWLW[&#39;HWLWcode&#39;]==1]
        data_LW = data_HWLW[data_HWLW[&#39;HWLWcode&#39;]==2]
        #create empty variables if not yet present
        
        #HW
        if &#39;time_HW&#39; not in ncdimlist:
            data_nc.createDimension(&#39;time_HW&#39;,len(data_HW))
        if &#39;time_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;time_HW&#39;,&#39;f8&#39;,(&#39;time_HW&#39;,))
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;time_HW&#39;][:] = date2num(data_HW.index.tolist(),units=data_nc.variables[&#39;time_HW&#39;].units)
        if &#39;waterlevel_astro_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_HW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;time_HW&#39;,))
            nc_newvar.setncatts(dict_HWattr)
        data_nc.variables[&#39;waterlevel_astro_HW&#39;][nstat,:] = data_HW[&#39;values&#39;]
        
        #LW
        if &#39;time_LW&#39; not in ncdimlist:
            data_nc.createDimension(&#39;time_LW&#39;,len(data_LW))
        if &#39;time_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;time_LW&#39;,&#39;f8&#39;,(&#39;time_LW&#39;,))
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;time_LW&#39;][:] = date2num(data_LW.index.tolist(),units=data_nc.variables[&#39;time_LW&#39;].units)
        if &#39;waterlevel_astro_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_LW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;time_LW&#39;,))
            nc_newvar.setncatts(dict_LWattr)
        data_nc.variables[&#39;waterlevel_astro_LW&#39;][nstat,:] = data_LW[&#39;values&#39;]
        
        #HWLW numbering
        if &#39;HWLWno&#39; in ts_ext.columns:
            if &#39;waterlevel_astro_HW_numbers&#39; not in ncvarlist:
                nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_HW_numbers&#39;,&#39;i4&#39;,(&#39;stations&#39;,&#39;time_HW&#39;,))
                #nc_newvar.setncatts(dict_HWattr)
            data_nc.variables[&#39;waterlevel_astro_HW_numbers&#39;][nstat,:] = data_HW[&#39;HWLWno&#39;]
            if &#39;waterlevel_astro_LW_numbers&#39; not in ncvarlist:
                nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_LW_numbers&#39;,&#39;i4&#39;,(&#39;stations&#39;,&#39;time_LW&#39;,))
                #nc_newvar.setncatts(dict_LWattr)
            data_nc.variables[&#39;waterlevel_astro_LW_numbers&#39;][nstat,:] = data_LW[&#39;HWLWno&#39;]
    
    data_nc.close()
    return


def write_tsdia(ts, station, vertref, filename, headerformat=&#39;dia&#39;):
    &#34;&#34;&#34;
    Writes the timeseries to an equidistant dia file

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    station : TYPE
        DESCRIPTION.
    vertref : TYPE
        DESCRIPTION.
    filename : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    
    if vertref == &#39;NAP&#39;:
        waarnemingssoort = 18
        vertreflong = &#39;T.o.v. Normaal Amsterdams Peil&#39;
    elif vertref == &#39;MSL&#39;:
        waarnemingssoort = 55
        vertreflong = &#39;T.o.v. Mean Sea Level&#39;
    else:
        raise Exception(&#39;ERROR: currently only vertref=&#34;NAP&#34; and vertref=&#34;MSL&#34; are supported for writing diafiles&#39;)
    grootheid = &#39;WATHTBRKD;Waterhoogte berekend;J&#39;
    ana = &#39;F012;Waterhoogte astronomisch mbv harmonische analyse&#39;
    
    time_today = dt.datetime.today().strftime(&#39;%Y%m%d&#39;)
    tstart_str = ts.index[0].strftime(&#39;%Y%m%d;%H%M&#39;)
    tstop_str = ts.index[-1].strftime(&#39;%Y%m%d;%H%M&#39;)
    timestep_min = (ts.index[1]-ts.index[0]).total_seconds()/60
    
    ts_values = ts[&#39;values&#39;]
    metadata_pd = pd.Series([&#39;[IDT;*DIF*;A;;%6s]&#39;%(time_today),
                             &#39;[W3H]&#39;,
                             &#39;WNS;%i&#39;%(waarnemingssoort),
                             &#39;PAR;%s&#39;%(grootheid), #parameter/grootheid, gelijk voor waarnemingssoorten 18 en 55
                             &#39;CPM;10;Oppervlaktewater&#39;, #compartiment, gelijk voor waarnemingssoorten 18 en 55
                             &#39;EHD;I;cm&#39;, #eenheid, gelijk voor waarnemingssoorten 18 en 55
                             &#39;HDH;%s;%s&#39;%(vertref,vertreflong),
                             ##&#39;ORG;NVT;Niet van toepassing&#39;,
                             ##&#39;SGK;NVT&#39;,
                             ##&#39;IVS;NVT;Niet van toepassing&#39;,
                             ##&#39;BTX;NVT;NVT;Niet van toepassing&#39;,
                             ##&#39;BTN;Niet van toepassing&#39;,
                             &#39;ANI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BHI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BMI;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;OGI;RIKZMON_WAT;RIKZ - Landelijke monitoring waterhoogten gegevens&#39;, #niet_essentieel?
                             ##&#39;GBD;NIEUWWTWG;Nieuwe Waterweg&#39;,
                             &#39;LOC;%s&#39;%(station), #;Hoek van Holland;P;RD;6793000;44400000
                             &#39;ANA;%s&#39;%(ana),
                             &#39;BEM;NVT&#39;, #niet_essentieel?
                             &#39;BEW;NVT&#39;, #niet_essentieel?
                             &#39;VAT;NVT&#39;, #niet_essentieel?
                             &#39;TYP;TE&#39;, #reekstype: equidistant
                             &#39;[RKS]&#39;,
                             &#39;TYD;%10s;%10s;%i;min&#39;%(tstart_str,tstop_str,timestep_min),
                             ##&#39;PLT;NVT;-999999999;6793000;44400000&#39;,
                             ##&#39;SYS;CENT&#39;,
                             &#39;[TPS]&#39;,
                             &#39;STA;%10s;%10s;O&#39;%(tstart_str,tstop_str),
                             &#39;[WRD]&#39;])
    if headerformat==&#39;wia&#39;:
        for metalinestart in [&#39;[IDT;&#39;,&#39;WNS&#39;]:
            bool_drop = metadata_pd.str.startswith(metalinestart)
            metadata_pd = metadata_pd[~bool_drop]
        metadata_pd[metadata_pd.str.startswith(&#39;PAR&#39;)] = &#39;GHD;%s&#39;%(grootheid)#.split(&#39;;&#39;)[0])
        metadata_pd[metadata_pd.str.startswith(&#39;CPM&#39;)] = &#39;CPM;OW;Oppervlaktewater&#39;
        metadata_pd[metadata_pd.str.startswith(&#39;ANA&#39;)] = &#39;WBM;other:%s&#39;%(ana)#.split(&#39;;&#39;)[0])
    
    linestr_list = []
    linestr = &#39;&#39;
    for iV, ts_value in enumerate(ts_values): # iterate over ts_values
        linestr_add = &#34;%i/0:&#34;%(np.round(ts_value*100))
        linestr = linestr + linestr_add
        if (len(linestr) &gt; 114) or (iV==len(ts_values)-1): # append linestr to linestr_list if linestr is longer than n characters or last item of ts_values was reached
            linestr_list.append(linestr)
            linestr = &#39;&#39;
    data_todia = pd.Series(linestr_list)
    
    with io.open(filename,&#39;w&#39;, newline=&#39;\n&#39;) as f: #open file and set linux newline style
        for metaline in metadata_pd:
            f.write(&#39;%s\n&#39;%(metaline))
        data_todia.to_csv(f,index=False,header=False)


def write_tsdia_HWLW(ts_ext, station, vertref, filename, headerformat=&#39;dia&#39;):
    &#34;&#34;&#34;
    writes the extremes timeseries to a non-equidistant dia file

    Parameters
    ----------
    ts_ext : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.
    station : TYPE
        DESCRIPTION.
    vertref : TYPE
        DESCRIPTION.
    filename : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    
    if vertref == &#39;NAP&#39;:
        waarnemingssoort = 18
        vertreflong = &#39;T.o.v. Normaal Amsterdams Peil&#39;
        parameterX = &#39;GETETBRKD2;Getijextreem berekend&#39;
    elif vertref == &#39;MSL&#39;:
        waarnemingssoort = 55
        vertreflong = &#39;T.o.v. Mean Sea Level&#39;
        parameterX = &#39;GETETBRKDMSL2;Getijextreem berekend t.o.v. MSL&#39;
    else:
        raise Exception(&#39;ERROR: currently only vertref=&#34;NAP&#34; and vertref=&#34;MSL&#34; are supported for writing diafiles&#39;)
    grootheid = &#39;WATHTBRKD;Waterhoogte berekend;J&#39;
    ana = &#39;F012;Waterhoogte astronomisch mbv harmonische analyse&#39; #HW en LW uit 1 min. waterhoogten gefilterd uit 10 min. gem.
    time_today = dt.datetime.today().strftime(&#39;%Y%m%d&#39;)
    tstart_str = ts_ext.index[0].strftime(&#39;%Y%m%d;%H%M&#39;)
    tstop_str = ts_ext.index[-1].strftime(&#39;%Y%m%d;%H%M&#39;)
    
    if 11 in ts_ext[&#39;HWLWcode&#39;].values or 22 in ts_ext[&#39;HWLWcode&#39;].values:
        raise Exception(&#39;ERROR: invalid HWLWcodes in provided extreme timeseries (11 and/or 22)&#39;)
    
    metadata_pd = pd.Series([&#39;[IDT;*DIF*;A;;%6s]&#39;%(time_today),
                             &#39;[W3H]&#39;,
                             &#39;MUX;%s&#39;%(parameterX),
                             ##IVS;NVT;Niet van toepassing
                             ##BTX;NVT;NVT;Niet van toepassing
                             ##BTN;Niet van toepassing
                             &#39;ANI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BHI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BMI;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;OGI;RIKZMON_WAT;RIKZ - Landelijke monitoring waterhoogten gegevens&#39;, #niet_essentieel?
                             ##GBD;NIEUWWTWG;Nieuwe Waterweg
                             &#39;LOC;%s&#39;%(station),
                             &#39;ANA;%s&#39;%(ana),
                             &#39;BEM;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;BEW;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;VAT;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;TYP;TN&#39;, #reekstype: niet-equidistant
                             &#39;[MUX]&#39;,
                             &#39;MXW;1;15&#39;,
                             &#39;MXP;1;GETETCDE;Getijextreem code;J&#39;,
                             &#39;MXC;1;10;Oppervlaktewater&#39;,
                             &#39;MXE;1;T;DIMSLS&#39;,
                             &#39;MXH;1;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;MXO;1;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;MXS;1;NVT&#39;, #niet_essentieel?
                             &#39;MXW;2;%i&#39;%(waarnemingssoort),
                             &#39;MXP;2;%s&#39;%(grootheid),
                             &#39;MXC;2;10;Oppervlaktewater&#39;,
                             &#39;MXE;2;I;cm&#39;,
                             &#39;MXH;2;%s;%s&#39;%(vertref, vertreflong),
                             &#39;MXO;2;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;MXS;2;NVT&#39;, #niet_essentieel?
                             &#39;[TYP]&#39;,
                             &#39;TVL;1;1;hoogwater&#39;,
                             &#39;TVL;1;2;laagwater&#39;,
                             &#39;TVL;1;3;laagwater 1&#39;,
                             &#39;TVL;1;4;topagger&#39;,
                             &#39;TVL;1;5;laagwater 2&#39;,
                             &#39;[RKS]&#39;,
                             &#39;TYD;%10s;%10s&#39;%(tstart_str,tstop_str),
                             ##&#39;PLT;NVT;-999999999;6793000;44400000&#39;,
                             &#39;SYS;CENT&#39;, #niet_essentieel?
                             &#39;[TPS]&#39;,
                             &#39;STA;%10s;%10s;O&#39;%(tstart_str,tstop_str),
                             &#39;[WRD]&#39;])
    if headerformat==&#39;wia&#39;:
        for metalinestart in [&#39;[IDT;&#39;,&#39;MXW&#39;]:
            bool_drop = metadata_pd.str.startswith(metalinestart)
            metadata_pd = metadata_pd[~bool_drop]
        metadata_pd[metadata_pd.str.startswith(&#39;ANA&#39;)] = &#39;WBM;other:%s&#39;%(ana)#.split(&#39;;&#39;)[0])
        metadata_pd[metadata_pd.str.startswith(&#39;MXP;1&#39;)] = &#39;MXT;1;GETETTPE&#39; # GETETCDE;Getijextreem code naar GETETTPE
        metadata_pd[metadata_pd.str.startswith(&#39;MXC;1&#39;)] = &#39;MXC;1;OW;Oppervlaktewater&#39;
        metadata_pd[metadata_pd.str.startswith(&#39;MXP;2&#39;)] = &#39;MXG;2;%s&#39;%(grootheid)
        metadata_pd[metadata_pd.str.startswith(&#39;MXC;2&#39;)] = &#39;MXC;2;OW;Oppervlaktewater&#39;

    data_todia = ts_ext.index.strftime(&#39;%Y%m%d;%H%M&#39;)+&#39;;&#39;+ts_ext[&#39;HWLWcode&#39;].astype(str)+&#39;/0;&#39;+(ts_ext[&#39;values&#39;]*100).round().astype(int).astype(str)+&#39;:&#39;

    with io.open(filename,&#39;w&#39;, newline=&#39;\n&#39;) as f: #open file and set linux newline style
        for metaline in metadata_pd:
            f.write(&#39;%s\n&#39;%(metaline))
        data_todia.to_csv(f,index=False,header=False)


def writets_noos(ts, filename, metadata=None):
    &#34;&#34;&#34;
    Writes the timeseries to a noos file

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    filename : TYPE
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    
    timestamp = dt.datetime.now().strftime(&#39;%c&#39;)
    ts_out = pd.DataFrame({&#39;times&#39;:ts.index.strftime(&#39;%Y%m%d%H%M&#39;),&#39;values&#39;:ts[&#39;values&#39;]})
    
    if metadata is None:
        header_txt = f&#34;&#34;&#34;------------------------------------------------------
        Timeseries written by hatyan
        Created at {timestamp}
        ------------------------------------------------------
        Location    : None
        Position    : None
        Source      : None
        Unit        : waterlevel
        Analyse time: 000000000000
        Timezone    : None
        ------------------------------------------------------&#34;&#34;&#34;
    else:
        header_txt = f&#34;&#34;&#34;------------------------------------------------------\nTimeseries written by hatyan\nCreated at {timestamp}\n------------------------------------------------------\n&#34;&#34;&#34;
        for key in metadata.keys():
            header_txt = header_txt+(&#39;%-12s: %s\n&#39;%(key, metadata[key]))
        header_txt = header_txt+&#39;------------------------------------------------------&#39;
    np.savetxt(filename,ts_out,fmt=&#39;%s %7.4f&#39;,header=header_txt)


def crop_timeseries(ts, times_ext, onlyfull=True):
    &#34;&#34;&#34;
    Crops the provided timeseries

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    times_ext : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    ts_pd_out : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    ts_pd_in = ts
    
    print(&#39;-&#39;*50)
    print(&#39;cropping timeseries&#39;)
    if not times_ext[0]&lt;times_ext[1]:
        raise Exception(&#39;ERROR: the two times times_ext should be increasing, but they are not: %s.&#39;%(times_ext))
    if (times_ext[0] &lt; ts_pd_in.index[0]) or (times_ext[-1] &gt; ts_pd_in.index[-1]):
        message = &#39;imported timeseries is not available within entire requested period:\nrequested period:    %s to %s\nimported timeseries: %s to %s&#39;%(times_ext[0],times_ext[-1],ts_pd_in.index[0],ts_pd_in.index[-1])
        if onlyfull:
            raise Exception(&#39;ERROR: %s&#39;%(message))
        else:
            print(&#39;WARNING: %s&#39;%(message))
            
    times_selected_bool = (ts_pd_in.index &gt;= times_ext[0]) &amp; (ts_pd_in.index &lt;= times_ext[-1])
    ts_pd_out = ts_pd_in.loc[times_selected_bool]
    
    print(check_ts(ts_pd_out))
    return ts_pd_out


def resample_timeseries(ts, timestep_min, tstart=None, tstop=None):
    &#34;&#34;&#34;
    resamples the provided timeseries, only overlapping timesteps are selected, so no interpolation. with tstart/tstop it is possible to extend the timeseries with NaN values.

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the timeseries to be resampled.
    timestep_min : int
        the amount of minutes with which to resample the timeseries.
    tstart : dt.datetime, optional
        the start date for the resampled timeseries, the default is None which results in using the start date of the input ts.
    tstop : dt.datetime, optional
        the stop date for the resampled timeseries, the default is None which results in using the stop date of the input ts.

    Returns
    -------
    data_pd_resample : pandas.DataFrame with a &#39;values&#39; column and a pd.DatetimeIndex as index
        the resampled timeseries.

    &#34;&#34;&#34;
    
    print(&#39;-&#39;*50)
    print(&#39;resampling timeseries to %i minutes&#39;%(timestep_min))
    
    bool_duplicated_index = ts.index.duplicated()
    if bool_duplicated_index.sum()&gt;0:
        raise Exception(&#39;there are duplicated values in the ts DatetimeIndex, this is not supported by Timeseries.resample_timeseries(). Try &#34;ts_nodupl = ts[~ts.index.duplicated()]&#34;&#39;)
    
    if tstart is None:
        tstart = ts.index[0]
    if tstop is None:
        tstop = ts.index[-1]
    data_pd_resample = pd.DataFrame({},index=pd.date_range(tstart,tstop,freq=&#39;%dmin&#39;%(timestep_min))) #generate timeseries with correct tstart/tstop and interval
    data_pd_resample[&#39;values&#39;] = ts[&#39;values&#39;] #put measurements into this timeseries, matches to correct index automatically
    
    print(check_ts(data_pd_resample))
    return data_pd_resample


def check_rayleigh(ts_pd,t_const_freq_pd):
    
    t_const_freq = t_const_freq_pd.sort_values(&#39;freq&#39;)[&#39;freq&#39;].drop(&#39;A0&#39;,errors=&#39;ignore&#39;)
    freq_diffs = np.diff(t_const_freq)
    rayleigh_tresh = 0.7 #0.99 # Koos Doekes: &#34;Bij het algoritme dat HATYAN gebruikt mag men in de praktijk het Rayleigh-criterium enigszins schenden, tot zo&#39;n 0,7 van de theoretisch vereiste reekslengte. &#34;
    rayleigh = len(ts_pd[&#39;values&#39;])*freq_diffs #TODO: might be better to drop timeseries nan-values first
    freq_diff_min = rayleigh_tresh/len(ts_pd[&#39;values&#39;])
    rayleigh_bool = rayleigh&gt;rayleigh_tresh
    rayleigh_bool_id = np.where(~rayleigh_bool)[0]
    
    if rayleigh_bool.all():
        print(&#39;Rayleigh criterion OK (always&gt;%.2f, minimum is %.2f)&#39;%(rayleigh_tresh, np.min(rayleigh)))
        print(&#39;Frequencies are far enough apart (always &gt;%.6f, minimum is %.6f)&#39;%(freq_diff_min,np.min(freq_diffs)))
    else:
        print(&#39;Rayleigh criterion vandalised (not always&gt;%.2f, minimum is %.2f)&#39;%(rayleigh_tresh, np.min(rayleigh)))
        print(&#39;Frequencies with not enough difference (not always &gt;%.6f, minimum is %.6f)&#39;%(freq_diff_min,np.min(freq_diffs)))
        for ray_id in rayleigh_bool_id:
            t_const_freq_sel = t_const_freq.iloc[[ray_id,ray_id+1]]
            t_const_freq_sel[&#39;diff&#39;] = np.diff(t_const_freq_sel.values)[0]
            print(t_const_freq_sel)
            if t_const_freq_sel[&#39;diff&#39;] &lt; 1e-9:
                print(&#39;WARNING: difference almost zero, will result in ill conditioned matrix&#39;)
        

def check_ts(ts):
    &#34;&#34;&#34;
    prints several statistics of the provided timeseries

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries to be checked.

    Returns
    -------
    print_statement: str
        For printing as a substring of another string.

    &#34;&#34;&#34;
    
    stats = Timeseries_Statistics(ts=ts)
    return stats
    
    #TODO: THE PART BELOW IS NOT USED
    raise Exception(&#39;use hatyan.Timeseries_Statistics() instead&#39;)
    
    timesteps_min_all = ts.index.to_series().diff()[1:].dt.total_seconds()/60
    bool_int = (timesteps_min_all-timesteps_min_all.round(0))&lt;1e-9
    if bool_int.all():
        timesteps_min_all = timesteps_min_all.astype(int)
    else: #in case of non integer minute timesteps (eg seconds)
        timesteps_min_all[bool_int] = timesteps_min_all[bool_int].round(0)
    timesteps_min = set(timesteps_min_all)
    #print(timesteps_min)
    if len(timesteps_min)&lt;=100:
        timesteps_min_print = timesteps_min
    else:
        timesteps_min_print = &#39;too much unique time intervals (&gt;100) to display all of them, %i intervals ranging from %i to %i minutes&#39;%(len(timesteps_min),np.min(list(timesteps_min)),np.max(list(timesteps_min)))
    if (timesteps_min_all&gt;0).all():
        timesteps_incr_print = &#39;all time intervals are in increasing order and are never equal&#39;
    else:
        timesteps_incr_print = &#39;the times-order of ts is not always increasing (duplicate values or wrong order)&#39;
    
    ntimes_nonan = ts[&#39;values&#39;].count()
    ntimes = len(ts)
    ntimesteps_uniq = len(timesteps_min)

    
    if len(ts)==0:
        print_statement = f&#39;timeseries contents:\n{ts}&#39;
    else:
        print_statement = (f&#39;timeseries contents:\n{ts}\n&#39;+
                           f&#39;timeseries # unique timesteps: {ntimesteps_uniq}\n&#39;+
                           f&#39;timeseries unique timesteps (minutes):\n{timesteps_min_print}\n&#39;+
                           f&#39;timeseries validity: {timesteps_incr_print}\n&#39;+
                           f&#39;timeseries length: {ntimes}\n&#39;+
                           f&#39;timeseries # nonan: {ntimes_nonan}\n&#39;+
                           f&#39;timeseries % nonan: {(ntimes_nonan/ntimes*100):.1f}%\n&#39;+
                           f&#39;timeseries # nan: {ntimes-ntimes_nonan}\n&#39;+
                           f&#39;timeseries % nan: {(ntimes-ntimes_nonan)/ntimes*100:.1f}%&#39;)
    return print_statement


class Timeseries_Statistics:
    #TODO: make like a dict with different __str__ method, instead of this mess https://stackoverflow.com/questions/4014621/a-python-class-that-acts-like-dict
    #TODO: improve output dict, keys are now not convenient to use. Maybe make keys and longname?
    def __init__(self,ts):
        timesteps_min_all = ts.index.to_series().diff()[1:].dt.total_seconds()/60
        bool_int = (timesteps_min_all-timesteps_min_all.round(0))&lt;1e-9
        if bool_int.all():
            timesteps_min_all = timesteps_min_all.astype(int)
        else: #in case of non integer minute timesteps (eg seconds)
            timesteps_min_all[bool_int] = timesteps_min_all[bool_int].round(0)
        timesteps_min = set(timesteps_min_all)
        #print(timesteps_min)
        if len(timesteps_min)&lt;=100:
            timesteps_min_print = timesteps_min
        else:
            timesteps_min_print = &#39;too much unique time intervals (&gt;100) to display all of them, %i intervals ranging from %i to %i minutes&#39;%(len(timesteps_min),np.min(list(timesteps_min)),np.max(list(timesteps_min)))
        if (timesteps_min_all&gt;0).all():
            timesteps_incr_print = &#39;all time intervals are in increasing order and are never equal&#39;
        else:
            timesteps_incr_print = &#39;the times-order of ts is not always increasing (duplicate values or wrong order)&#39;
        
        ntimes_nonan = ts[&#39;values&#39;].count()
        ntimes = len(ts)
        ntimesteps_uniq = len(timesteps_min)
        if len(ts)==0:
            self.stats = {&#39;timeseries contents&#39;:ts}
        else:
            self.stats = {&#39;timeseries contents&#39;:ts,
                        &#39;timeseries # unique timesteps&#39;: ntimesteps_uniq,
                        &#39;timeseries unique timesteps (minutes)&#39;:timesteps_min_print,
                        &#39;timeseries validity&#39;: timesteps_incr_print,
                        &#39;timeseries length&#39;: ntimes,
                        &#39;timeseries # nonan&#39;: ntimes_nonan,
                        &#39;timeseries % nonan&#39;: ntimes_nonan/ntimes*100,#%.1f %
                        &#39;timeseries # nan&#39;: ntimes-ntimes_nonan,
                        &#39;timeseries % nan&#39;: (ntimes-ntimes_nonan)/ntimes*100, #%.1f %
                        }
    def __str__(self):
        print_statement = &#39;&#39;
        for key in self.stats.keys():
            if key in [&#39;timeseries contents&#39;,&#39;timeseries unique timesteps (minutes)&#39;]:
                print_statement += f&#39;{key}:\n{self.stats[key]}\n&#39;
            else:
                print_statement += f&#39;{key}: {self.stats[key]}\n&#39;
        return print_statement
    def __repr__(self): #avoid printing the class name
        #return dict.__repr__
        return str(self.stats)
    &#34;&#34;&#34;
    @classmethod
    def keys(self):
        return self.stats.keys()
    &#34;&#34;&#34;
        
    
    
###############################
################# READING FILES
###############################


def get_diablocks_startstopstation(filename):
    &#34;&#34;&#34;
    Gets information about the data blocks present in a dia file

    Parameters
    ----------
    filename : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    diablocks_pd_startstopstation: pd.DataFrame
        Pandas DataFrame with &#39;block_starts&#39;,&#39;data_starts&#39;,&#39;data_ends&#39;,&#39;station&#39;

    &#34;&#34;&#34;
    
    #get list of starts/ends of datasets in diafile
    linenum_colnames = [&#39;block_starts&#39;,&#39;data_starts&#39;,&#39;data_ends&#39;]
    diablocks_pd_startstopstation = pd.DataFrame({},columns=linenum_colnames)
    
    with open(filename, encoding=&#39;latin1&#39;) as f: #&#39;latin1 is nodig om predictie diafile die rechtstreeks uit hatyan komen in te lezen (validatietijdserie met op regel 4 (PAR) ongeldige tekens aan het einde)
        block_id = -1
        for linenum, line in enumerate(f, 1):
            if &#39;[W3H]&#39; in line:
                block_id += 1
                diablocks_pd_startstopstation.loc[block_id,&#39;block_starts&#39;] = linenum
            elif &#39;[WRD]&#39; in line:
                diablocks_pd_startstopstation.loc[block_id,&#39;data_starts&#39;] = linenum
            elif &#39;LOC&#39; in line:
                diablocks_pd_startstopstation.loc[block_id,&#39;station&#39;] = line.rstrip().split(&#39;;&#39;)[1]
    diablocks_pd_startstopstation[&#39;data_ends&#39;] = (diablocks_pd_startstopstation[&#39;block_starts&#39;]-1).tolist()[1:]+[linenum]
    if diablocks_pd_startstopstation.isnull().any().any():
        raise Exception(&#39;ERROR: multiple blocks in diafile, but unequal amount of start/end/datastart/stationnames&#39;)
    
    #convert columns with line numbers to integers
    diablocks_pd_startstopstation[linenum_colnames] = diablocks_pd_startstopstation[linenum_colnames].astype(int)
    
    return diablocks_pd_startstopstation


def get_diablocks(filename):
    
    print(&#39;reading file: %s&#39;%(filename))
    diablocks_pd = get_diablocks_startstopstation(filename)
    for block_id in diablocks_pd.index.tolist():
        #read diafile metadata as pandas series, prevent splitting of combined paramater names like MXH;2 by replacing ; with !
        data_meta_nrows = diablocks_pd.loc[block_id,&#39;data_starts&#39;] - diablocks_pd.loc[block_id,&#39;block_starts&#39;]
        data_meta_series = pd.read_table(filename,skiprows=diablocks_pd.loc[block_id,&#39;block_starts&#39;],nrows=data_meta_nrows,header=None)[0] #series of metadata
        if not data_meta_series.str.contains(&#39;GHD|MXG;2&#39;).any(): #wia files contain these parameters, dia files don&#39;t. Replace dia names with wia names (wia files also contain PAR and MXP;2, but they should not be replaced)
            data_meta_series = data_meta_series.str.replace(&#39;PAR&#39;,&#39;GHD&#39;).str.replace(&#39;MXP;2&#39;,&#39;MXG;2&#39;)
        bool_combinedparname = (data_meta_series.str[3:6]==&#39;;1;&#39;) | (data_meta_series.str[3:6]==&#39;;2;&#39;)
        data_meta_series.loc[bool_combinedparname] = data_meta_series.loc[bool_combinedparname].str.slice_replace(3,4,&#39;!&#39;)
        
        #get groepering and whether dia/wia is equidistant or non-equidistant
        bool_startswithmux = data_meta_series.str.startswith(&#39;MUX&#39;)
        row_TYP = data_meta_series.loc[data_meta_series.str.startswith(&#39;TYP&#39;)].iloc[0].split(&#39;;&#39;)[1]
        diablocks_pd.loc[block_id,&#39;TYP&#39;] = row_TYP
        if row_TYP==&#39;TN&#39;: #bool_startswithmux.any(): #extreme waterlevel timeseries (non-equidistant)
            mincontent = [&#39;MXG;2&#39;,&#39;LOC&#39;,&#39;MXH;2&#39;,&#39;MXE;2&#39;,&#39;TYD&#39;,&#39;STA&#39;]
            diablocks_pd.loc[block_id,&#39;groepering&#39;] = data_meta_series.loc[bool_startswithmux].iloc[0].split(&#39;;&#39;)[1]
        elif row_TYP==&#39;TE&#39;: #normal waterlevel timeseries (equidistant)
            mincontent = [&#39;GHD&#39;,  &#39;LOC&#39;,&#39;HDH&#39;,  &#39;EHD&#39;,  &#39;TYD&#39;,&#39;STA&#39;] #WNS,CPM,HDH,ANA
            diablocks_pd.loc[block_id,&#39;groepering&#39;] = &#39;NVT&#39;
        else:
            raise Exception(f&#39;TYP &#34;{row_TYP}&#34; not implemented in hatyan.readts_dia()&#39;)
        
        #read all required metadata
        for get_content_sel in mincontent:
            bool_mincontent = data_meta_series.str.replace(&#39;!&#39;,&#39;;&#39;).str.startswith(get_content_sel)
            if bool_mincontent.sum()!=1:
                if get_content_sel!=&#39;STA&#39;:
                    raise Exception(f&#39;unexpected amount of matched metadatalines ({bool_mincontent.sum()}) for {get_content_sel}&#39;)
            data_meta_mincontent = data_meta_series.loc[bool_mincontent].iloc[0].split(&#39;;&#39;) #list type
            if get_content_sel in [&#39;GHD&#39;,&#39;MXG;2&#39;]: # Grootheid (dia/wia, dia/wia equidistant). Originally dia contains PAR and MXP;2, but they are replaced
                file_grootheidname = data_meta_mincontent[1]
                valid_grootheidnames = [&#39;WATHTE&#39;,&#39;WATHTBRKD&#39;,&#39;NVT&#39;] #NVT in wia files
                if file_grootheidname not in valid_grootheidnames:
                    raise Exception(&#39;ERROR: grootheid name (%s) should be in %s but is %s&#39;%(get_content_sel, valid_grootheidnames, file_grootheidname))
                diablocks_pd.loc[block_id,&#39;grootheid&#39;] = file_grootheidname
            elif get_content_sel in [&#39;LOC&#39;]: # Locatie. same in all files
                coords_pd = pd.DataFrame({&#39;epsg_in&#39;:[28992,4326,4230], &#39;factor&#39;:[100,1000000,1000000]}, index=[&#39;RD&#39;,&#39;W84&#39;,&#39;E50&#39;])
                if len(data_meta_mincontent)&lt;7:
                    print(&#39;no coordinate data available in LOC line of dia file&#39;)
                    continue
                coordsys_str, coord_x, coord_y = data_meta_mincontent[4:]
                if coordsys_str not in coords_pd.index:
                    raise Exception(&#39;unknown coordinate system string in diafile ({coordsys_str})&#39;)
                diablocks_pd.loc[block_id,&#39;x&#39;] = int(coord_x)/coords_pd.loc[coordsys_str,&#39;factor&#39;]
                diablocks_pd.loc[block_id,&#39;y&#39;] = int(coord_y)/coords_pd.loc[coordsys_str,&#39;factor&#39;]
                diablocks_pd.loc[block_id,&#39;coordsys&#39;] = coordsys_str
                diablocks_pd.loc[block_id,&#39;epsg&#39;] = coords_pd.loc[coordsys_str,&#39;epsg_in&#39;]
            elif get_content_sel in [&#39;EHD&#39;,&#39;MXE;2&#39;]: # Eenheid. equidistant dia/wia, non-equidistant dia/wia
                file_eenheid = data_meta_mincontent[2]
                if file_eenheid != &#39;cm&#39;:
                    raise Exception(&#39;unknown eenheid in diafile: %s&#39;%(file_eenheid))
                diablocks_pd.loc[block_id,&#39;eenheid&#39;] = file_eenheid
            elif get_content_sel in [&#39;HDH&#39;,&#39;MXH;2&#39;]: # Hoedanigheid (NAP/MSL). equidistant dia/wia, non-equidistant dia/wia
                diablocks_pd.loc[block_id,&#39;vertref&#39;] = data_meta_mincontent[1]
            elif get_content_sel in [&#39;TYD&#39;]: #Tijdstip. same in all files
                datestart = dt.datetime.strptime(data_meta_mincontent[1]+data_meta_mincontent[2], &#34;%Y%m%d%H%M&#34;)
                datestop = dt.datetime.strptime(data_meta_mincontent[3]+data_meta_mincontent[4], &#34;%Y%m%d%H%M&#34;)
                if len(data_meta_mincontent)==5: #nonequidistant timeseries
                    timestep_value = None
                elif len(data_meta_mincontent)==7: #equidistant timeseries contains also timeunit and timestep
                    timestep_unit = data_meta_mincontent[6]
                    if timestep_unit != &#39;min&#39;:
                        raise Exception(&#39;ERROR: time unit from TYD is in unknown format (not &#34;min&#34;)&#39;)
                    timestep_value = int(data_meta_mincontent[5]) #int(timestep_value_raw)
                else:
                    raise Exception(f&#39;ERROR: time metadata is not understood: {data_meta_mincontent}&#39;)
                diablocks_pd.loc[block_id,&#39;tstart&#39;] = datestart
                diablocks_pd.loc[block_id,&#39;tstop&#39;] = datestop
                diablocks_pd.loc[block_id,&#39;timestep_min&#39;] = timestep_value
            elif get_content_sel in [&#39;STA&#39;]: #Status. same in all files
                diablocks_pd.loc[block_id,&#39;STA&#39;] = &#39;!&#39;.join(data_meta_series.loc[bool_mincontent].tolist())
    return diablocks_pd


def readts_dia_nonequidistant(filename, diablocks_pd, block_id):

    data_nrows = diablocks_pd.loc[block_id,&#39;data_ends&#39;] - diablocks_pd.loc[block_id,&#39;data_starts&#39;]
    data_pd_HWLW = pd.read_csv(filename,skiprows=diablocks_pd.loc[block_id,&#39;data_starts&#39;],nrows=data_nrows, header=None, names=[&#39;date&#39;,&#39;time&#39;,&#39;HWLWcode/qualitycode&#39;,&#39;valuecm:&#39;], sep=&#39;;&#39;, parse_dates={&#39;times&#39;:[0,1]})
    
    #convert HWLW+quality code to separate columns
    data_pd_HWLWtemp = data_pd_HWLW.loc[:,&#39;HWLWcode/qualitycode&#39;].str.split(&#39;/&#39;, expand=True)
    data_pd_HWLW[&#39;HWLWcode&#39;] = data_pd_HWLWtemp.iloc[:,0].astype(&#39;int&#39;)
    data_pd_HWLW[&#39;qualitycode&#39;] = data_pd_HWLWtemp.iloc[:,1].astype(&#39;int&#39;)
    data_pd_HWLW = data_pd_HWLW.drop(&#39;HWLWcode/qualitycode&#39;,axis=&#39;columns&#39;)

    #convert value from cm to m
    data_pd_HWLW[&#39;values&#39;] = data_pd_HWLW[&#39;valuecm:&#39;].str.strip(&#39;:&#39;).astype(&#39;int&#39;)/100
    data_pd_HWLW = data_pd_HWLW.drop(&#39;valuecm:&#39;,axis=&#39;columns&#39;)
    
    bool_hiaat = data_pd_HWLW[&#39;qualitycode&#39;] == 99
    data_pd_HWLW.loc[bool_hiaat,&#39;values&#39;] = np.nan
    
    data_pd = data_pd_HWLW
    data_pd = data_pd.set_index(&#39;times&#39;)
    
    return data_pd


def readts_dia_equidistant(filename, diablocks_pd, block_id):
    
    datestart = diablocks_pd.loc[block_id,&#39;tstart&#39;]
    datestop = diablocks_pd.loc[block_id,&#39;tstop&#39;]
    timestep_min = diablocks_pd.loc[block_id,&#39;timestep_min&#39;]
    times_fromfile = pd.date_range(start=datestart,end=datestop,freq=&#39;%dmin&#39;%(timestep_min))
    
    #get data for station
    data_nrows = diablocks_pd.loc[block_id,&#39;data_ends&#39;] - diablocks_pd.loc[block_id,&#39;data_starts&#39;]
    data_pd = pd.read_csv(filename,skiprows=diablocks_pd.loc[block_id,&#39;data_starts&#39;],nrows=data_nrows, header=None)
    data_pdser = data_pd[0].str.strip()
    data = data_pdser.str.cat()
    data = data.strip(&#39;:&#39;) #remove first and/or last colon if present
    data = data.split(&#39;:&#39;)
    
    if len(times_fromfile) != len(data):
        raise Exception(&#39;ERROR: times and values ts are not of equal length\nlen(times_fromfile): %d\nlen(data): %d&#39;%(len(times_fromfile),len(data)))
    data_pd = pd.DataFrame({&#39;times&#39;:times_fromfile,&#39;valuecm/qualitycode&#39;:data})
    
    #convert HWLW+quality code to separate columns
    data_pd_temp = data_pd.loc[:,&#39;valuecm/qualitycode&#39;].str.split(&#39;/&#39;, expand=True)
    data_pd[&#39;values&#39;] = data_pd_temp.iloc[:,0].astype(&#39;int&#39;)/100
    data_pd[&#39;qualitycode&#39;] = data_pd_temp.iloc[:,1].astype(&#39;int&#39;)
    data_pd = data_pd.drop(&#39;valuecm/qualitycode&#39;,axis=&#39;columns&#39;)

    bool_hiaat = data_pd[&#39;qualitycode&#39;] == 99
    data_pd.loc[bool_hiaat,&#39;values&#39;] = np.nan
    data_pd = data_pd.set_index(&#39;times&#39;)
    
    return data_pd


def readts_dia(filename, station=None, block_ids=None, get_status=False):
    &#34;&#34;&#34;
    Reads an equidistant or non-equidistant dia file, or a list of dia files. Also works for diafiles containing multiple blocks for one station.

    Parameters
    ----------
    filename : TYPE
        DESCRIPTION.
    station : TYPE
        DESCRIPTION. The default is None.
    block_ids : int, list of int or &#39;allstation&#39;, optional
        DESCRIPTION. The default is None.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    data_pd : pandas.core.frame.DataFrame
        DataFrame with a &#39;values&#39; column and a pd.DatetimeIndex as index in case of an equidistant file, or more columns in case of a non-equidistant file.

    &#34;&#34;&#34;
    
    if not isinstance(filename,list):
        filename = [filename]
    if len(filename)==0:
        raise Exception(&#39;ERROR: filename list is empty&#39;)
    
    data_pd_all = pd.DataFrame()
    for iF, filename_one in enumerate(filename):    
        diablocks_pd = get_diablocks(filename_one)
        pd.set_option(&#39;display.max_columns&#39;, 6) #default was 0, but need more to display groepering
        pd.set_option(&#39;display.width&#39;, 200) #default was 80, but need more to display groepering
        print_cols = [&#39;block_starts&#39;, &#39;station&#39;, &#39;grootheid&#39;, &#39;groepering&#39;, &#39;tstart&#39;, &#39;tstop&#39;]
        print(&#39;blocks in diafile:\n%s&#39;%(diablocks_pd[print_cols]))
        str_getdiablockspd = &#39;A summary of the available blocks is printed above, obtain a full DataFrame of available diablocks with &#34;diablocks_pd=Timeseries.get_diablocks(filename)&#34;&#39;
        
        #get equidistant timeseries from metadata
        if block_ids is None or block_ids==&#39;allstation&#39;:
            if station is None:
                raise Exception(&#39;ERROR: if block_ids argument is not provided (or None) or is &#34;allstation&#34;, station argument should be provided.&#39;)
            bool_station = diablocks_pd[&#39;station&#39;]==station
            ids_station = diablocks_pd[bool_station].index.tolist()
            if len(ids_station)&lt;1:
                raise Exception(&#39;ERROR: no data block with requested station (%s) present in dia file. %s&#39;%(station, str_getdiablockspd))
            elif len(ids_station)&gt;1 and block_ids is None:
                raise Exception(&#39;ERROR: more than one data block with requested station (%s) present in dia file. Provide block_ids argument to readts_dia() (int, list of int or &#34;allstation&#34;). %s&#39;%(station, str_getdiablockspd))
            else: #exactly one occurrence or block_ids is provided or block_ids=&#39;allstation&#39;
                block_ids = ids_station
        
        #check validity of blockids of type listlist
        if isinstance(block_ids,int):
            block_ids = [block_ids]
        if not isinstance(block_ids,list):
            raise Exception(&#39;ERROR: invalid type for block_ids (should be int, list of int or &#34;allstation&#34;)&#39;)
        if not pd.Series(block_ids).isin(diablocks_pd.index).all():
            raise Exception(f&#39;ERROR: invalid values in block_ids list ({block_ids}), possible are {diablocks_pd.index.tolist()} (all integers)&#39;)
            
        if station is not None:
            if not isinstance(station,str):
                raise Exception(&#39;ERROR: station argument should be of type string&#39;)
            bool_samestation = diablocks_pd.loc[block_ids,&#39;station&#39;]==station
            if not bool_samestation.all():
                raise Exception(&#39;ERROR: both the arguments station and block_ids are provided, but at least one of the requested block_ids corresponds to a different station. %s&#39;%(str_getdiablockspd))
            
        data_pd_allblocks = pd.DataFrame()
        for block_id in block_ids:
            if np.isnan(diablocks_pd.loc[block_id,&#39;timestep_min&#39;]): #non-equidistant
                data_pd_oneblock = readts_dia_nonequidistant(filename_one, diablocks_pd, block_id)
            else: #equidistant
                data_pd_oneblock = readts_dia_equidistant(filename_one, diablocks_pd, block_id)
            if get_status: #TODO: this can be more generic (eg add additional metadata) or more neat. Also in get_diablocks()
                block_status_list = diablocks_pd.loc[block_id,&#39;STA&#39;].split(&#39;!&#39;)
                for block_status_one in block_status_list:
                    status_tstart = dt.datetime.strptime(block_status_one[4:17],&#39;%Y%m%d;%H%M&#39;)
                    status_tstop = dt.datetime.strptime(block_status_one[18:31],&#39;%Y%m%d;%H%M&#39;)
                    status_val = block_status_one[-1]
                    data_pd_oneblock.loc[status_tstart:status_tstop,&#39;Status&#39;] = status_val
            data_pd_allblocks = data_pd_allblocks.append(data_pd_oneblock, ignore_index=False)
        
        #append to allyears dataset
        data_pd_all = data_pd_all.append(data_pd_allblocks, ignore_index=False)

    #check overlapping timesteps, sort values on time and check_ts
    if len(data_pd_all) != len(data_pd_all.index.unique()):
        raise Exception(&#39;ERROR: merged datasets have duplicate/overlapping timesteps, clean up your input data or provide one file instead of a list&#39;)
    data_pd_all = data_pd_all.sort_index(axis=0)
    print(check_ts(data_pd_all))
    
    return data_pd_all


def readts_noos(filename, datetime_format=&#39;%Y%m%d%H%M&#39;, na_values=None):
    &#34;&#34;&#34;
    Reads a noos file

    Parameters
    ----------
    filename : TYPE
        DESCRIPTION.
    datetime_format : TYPE, optional
        DESCRIPTION. The default is &#39;%Y%m%d%H%M&#39;.
    na_values : TYPE, optional
        DESCRIPTION. The default is None.

    Returns
    -------
    data_pd : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    
    print(&#39;-&#39;*50)
    print(&#39;reading file: %s&#39;%(filename))
    noosheader = []
    noosheader_dict = {}
    with open(filename) as f:
        for linenum, line in enumerate(f, 0):
            if &#39;#&#39; in line:
                noosheader.append(line)
                comment_stripped = line.strip(&#39;#&#39;).strip().split(&#39;: &#39;)
                if len(comment_stripped) == 1:
                    if comment_stripped[0] != &#39;&#39;:
                        noosheader_dict[comment_stripped[0]] = &#39;&#39;
                else:
                    noosheader_dict[comment_stripped[0].strip()] = comment_stripped[1].strip()
            else:
                startdata = linenum
                break
    
    content_pd = pd.read_csv(filename,header=startdata-1,delim_whitespace=True,names=[&#39;times_str&#39;,&#39;values&#39;], na_values=na_values)
    noos_datetime = pd.to_datetime(content_pd[&#39;times_str&#39;],format=datetime_format)
    data_pd = pd.DataFrame({&#39;values&#39;:content_pd[&#39;values&#39;].values},index=noos_datetime)
    
    print(check_ts(data_pd))
    return data_pd</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hatyan.timeseries.calc_HWLW"><code class="name flex">
<span>def <span class="ident">calc_HWLW</span></span>(<span>ts, calc_HWLW345=False, calc_HWLW1122=False, debug=False, buffer_hr=6)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates extremes (high and low waters) for the provided timeseries.
This definition uses scipy.signal.find_peaks() with arguments 'distance' and 'prominence'.
The minimal 'distance' between two high or low water peaks is based on the M2 period: 12.42/1.5=8.28 hours for HW and 12.42/1.7=7.30 hours for LW (larger because of aggers).
The prominence for local extremes is set to 0.01m, to filter out very minor dips in the timeseries.
If there are two equal high or low water values, the first one is taken.
There are no main high/low waters calculated within 6 hours of the start/end of the timeseries (keyword buffer_hr), since these can be invalid.
This function can deal with gaps. Since scipy.signal.find_peaks() warns about nan values, those are removed first.
This does influence the results since find_peaks does not know about time registration. This is also tricky for input timeseries with varying time interval.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries with a tidal prediction or water level measurements.</dd>
<dt><strong><code>calc_HWLW345</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether to also calculate local extremes, first/second low waters and 'aggers'.
The default is False, in which case only extremes per tidal period are calculated.
When first/second low waters and aggers are calculated, the local extremes around highwater (eg double highwaters and dips) are filtered out first.</dd>
<dt><strong><code>calc_HWLW345_cleanup1122</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether to remove HWLWcodes 11 and 22 from DataFrame. The default is True.</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether to print debug information. The default is False.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_pd_HWLW</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame contains colums 'times', 'values' and 'HWLWcode', it contains the times, values and codes of the timeseries that are extremes.
1 (high water) and 2 (low water). And if calc_HWLW345=True also 3 (first low water), 4 (agger) and 5 (second low water).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_HWLW(ts, calc_HWLW345=False, calc_HWLW1122=False, debug=False, buffer_hr=6):
    &#34;&#34;&#34;
    
    Calculates extremes (high and low waters) for the provided timeseries. 
    This definition uses scipy.signal.find_peaks() with arguments &#39;distance&#39; and &#39;prominence&#39;. 
    The minimal &#39;distance&#39; between two high or low water peaks is based on the M2 period: 12.42/1.5=8.28 hours for HW and 12.42/1.7=7.30 hours for LW (larger because of aggers). 
    The prominence for local extremes is set to 0.01m, to filter out very minor dips in the timeseries.
    If there are two equal high or low water values, the first one is taken. 
    There are no main high/low waters calculated within 6 hours of the start/end of the timeseries (keyword buffer_hr), since these can be invalid.
    This function can deal with gaps. Since scipy.signal.find_peaks() warns about nan values, those are removed first.
    This does influence the results since find_peaks does not know about time registration. This is also tricky for input timeseries with varying time interval.
    
    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries with a tidal prediction or water level measurements.
    calc_HWLW345 : boolean, optional
        Whether to also calculate local extremes, first/second low waters and &#39;aggers&#39;. 
        The default is False, in which case only extremes per tidal period are calculated.
        When first/second low waters and aggers are calculated, the local extremes around highwater (eg double highwaters and dips) are filtered out first.
    calc_HWLW345_cleanup1122 : boolean, optional
        Whether to remove HWLWcodes 11 and 22 from DataFrame. The default is True.
    debug : boolean, optional
        Whether to print debug information. The default is False.
    
    Raises
    ------
    Exception
        DESCRIPTION.
    
    Returns
    -------
    data_pd_HWLW : pandas.DataFrame
        The DataFrame contains colums &#39;times&#39;, &#39;values&#39; and &#39;HWLWcode&#39;, it contains the times, values and codes of the timeseries that are extremes.
        1 (high water) and 2 (low water). And if calc_HWLW345=True also 3 (first low water), 4 (agger) and 5 (second low water).

    &#34;&#34;&#34;
    
    if not ts.index.is_monotonic_increasing:
        raise Exception(&#39;ERROR: timeseries is not monotonic increasing, supply sorted timeseries (ts = ts.index.sort_index()&#39;) #otherwise &#34;ValueError: &#39;list&#39; argument must have no negative elements&#34;
    
    #calculate the amount of steps in a M2 period, based on the most occurring timestep 
    M2_period_min = get_schureman_freqs([&#39;M2&#39;]).loc[&#39;M2&#39;,&#39;period [hr]&#39;]*60
    ts_steps_min_most = np.argmax(np.bincount((ts.index.to_series().diff().iloc[1:].dt.total_seconds()/60).astype(int).values))
    if ts_steps_min_most &gt; 1:
        print(&#39;WARNING: the timestep of the series for which to calculate extremes/HWLW is %i minutes, but 1 minute is recommended&#39;%(ts_steps_min_most))
    M2period_numsteps = M2_period_min/ts_steps_min_most
    
    data_pd_HWLW = pd.DataFrame({&#39;times&#39;:ts.index,&#39;values&#39;:ts[&#39;values&#39;],&#39;HWLWcode&#39;:np.nan}).reset_index(drop=True)
    #create empty HWLW dataframe
    if data_pd_HWLW[&#39;values&#39;].isnull().any():
        data_pd_HWLW = data_pd_HWLW[~data_pd_HWLW[&#39;values&#39;].isnull()].reset_index(drop=True)
        print(&#39;WARNING: the provided ts for extreme/HWLW calculation contained NaN values. To avoid unexpected results from scipy.signal.find_peaks(), the %i NaN values were removed from the ts (%.2f%%) before calculating extremes/HWLW.&#39;%(len(ts)-len(data_pd_HWLW), (len(ts)-len(data_pd_HWLW))/len(ts)*100))

    if calc_HWLW345 or calc_HWLW1122:
        #get all local extremes, including aggers and second high waters (1/2/11/22) #takes first value of two equal peaks, prominence naar 0.01 om matige aggers uit te sluiten
        LWid_all, LWid_all_properties = ssig.find_peaks(-data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=None)
        HWid_all, HWid_all_properties = ssig.find_peaks(data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=None)
        data_pd_HWLW.loc[LWid_all,&#39;HWLWcode&#39;] = 22 #all LW
        data_pd_HWLW.loc[HWid_all,&#39;HWLWcode&#39;] = 11 #all HW

    #get HWLW (extremes per tidal period). 
    LWid_main_raw,LWid_main_properties = ssig.find_peaks(-data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=M2period_numsteps/1.7) #most stations work with factor 1.4. 1.5 results in all LW values for HoekvanHolland for 2000, 1.7 results in all LW values for Rotterdam for 2000 (also for 1999-2002).
    HWid_main_raw,HWid_main_properties = ssig.find_peaks(data_pd_HWLW[&#39;values&#39;].values, prominence=(0.01,None), width=(None,None), distance=M2period_numsteps/1.9) #most stations work with factor 1.4. 1.5 value results in all HW values for DenHelder for year 2000 (also for 1999-2002). 1.7 results in all HW values for LITHDP 2018. 1.9 results in all correct values for LITHDP 2022
    # remove main extremes within 6 hours of start/end of timeseries, since they are often missed or invalid.
    validtimes_idx = data_pd_HWLW.loc[(data_pd_HWLW[&#39;times&#39;]&gt;=ts.index[0]+dt.timedelta(hours=buffer_hr)) &amp; (data_pd_HWLW[&#39;times&#39;]&lt;=ts.index[-1]-dt.timedelta(hours=buffer_hr))].index
    LWid_main = LWid_main_raw[np.in1d(LWid_main_raw,validtimes_idx)]
    HWid_main = HWid_main_raw[np.in1d(HWid_main_raw,validtimes_idx)]
    #use valid values to continue process
    data_pd_HWLW.loc[LWid_main,&#39;HWLWcode&#39;] = 2
    data_pd_HWLW.loc[HWid_main,&#39;HWLWcode&#39;] = 1
    
    #drop all non-(local)extreme timesteps and convert HWLWcode column to integers
    data_pd_HWLW = data_pd_HWLW.dropna(subset=[&#39;HWLWcode&#39;])
    data_pd_HWLW[&#39;HWLWcode&#39;] = data_pd_HWLW[&#39;HWLWcode&#39;].astype(int)

    if debug: #debug statistics
        prop_list = [&#39;prominences&#39;,&#39;widths&#39;]
        data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==2,prop_list] = pd.DataFrame(LWid_main_properties,index=LWid_main_raw).loc[LWid_main,prop_list]
        print(&#39;LW values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==2]))
        data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==1,prop_list] = pd.DataFrame(HWid_main_properties,index=HWid_main_raw).loc[HWid_main,prop_list]
        print(&#39;HW values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==1]))
        if calc_HWLW345 or calc_HWLW1122:
            LW_local_bool = ~np.in1d(LWid_all, LWid_main)
            data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==22,prop_list] = pd.DataFrame(LWid_all_properties,index=LWid_all).loc[LW_local_bool,prop_list]
            print(&#39;LW_local values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==22]))
            HW_local_bool = ~np.in1d(HWid_all, HWid_main)
            data_pd_HWLW.loc[data_pd_HWLW[&#39;HWLWcode&#39;]==11,prop_list] = pd.DataFrame(HWid_all_properties,index=HWid_all).loc[HW_local_bool,prop_list]
            print(&#39;HW_local values:\n%s\n&#39;%(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==11]))
    
    if calc_HWLW345: #recalculate local LW/HWs between two main HWs to firstLW/agger/secondLW
        data_pd_HWLW = calc_HWLWlocalto345(data_pd_HWLW,HWid_main)
    
    #return to normal time-index
    data_pd_HWLW = data_pd_HWLW.set_index(&#39;times&#39;)
    return data_pd_HWLW</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.calc_HWLWlocalto345"><code class="name flex">
<span>def <span class="ident">calc_HWLWlocalto345</span></span>(<span>data_pd_HWLW, HWid_main)</span>
</code></dt>
<dd>
<div class="desc"><p>Recalculate local LW/HWs between two main HWs to firstLW/agger/secondLW</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_pd_HWLW</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>HWid_main</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_pd_HWLW</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_HWLWlocalto345(data_pd_HWLW,HWid_main):
    &#34;&#34;&#34;
    Recalculate local LW/HWs between two main HWs to firstLW/agger/secondLW

    Parameters
    ----------
    data_pd_HWLW : TYPE
        DESCRIPTION.
    HWid_main : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    data_pd_HWLW : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    
    print(&#39;calculating 1stLW/agger/2ndLW for all tidalperiods (between two HW values)...&#39;)
    for iTide, dummy in enumerate(HWid_main[:-1]):
        data_pd_HWLW_1tide = data_pd_HWLW.loc[HWid_main[iTide]:HWid_main[iTide+1],:]
        
        #filter local extremes around HW (only interested in aggers, so LW), this is necessary for eg DENHDR and PETTZD, otherwise second HW is seen as first LW
        data_pd_HWLW_1tide_minHW = data_pd_HWLW_1tide.loc[data_pd_HWLW_1tide[&#39;HWLWcode&#39;]==1,[&#39;values&#39;]].min()[0]
        data_pd_HWLW_1tide_min = data_pd_HWLW_1tide[&#39;values&#39;].min()
        data_pd_HWLW_1tide_mid = np.mean([data_pd_HWLW_1tide_minHW,data_pd_HWLW_1tide_min])
        bool_LWs = data_pd_HWLW_1tide[&#39;values&#39;]&lt;data_pd_HWLW_1tide_mid
        data_pd_HWLW_1tide_noHWs = data_pd_HWLW_1tide[bool_LWs]
        
        if len(data_pd_HWLW_1tide_noHWs) &gt; 3: #(attempt to) reduce to three values between two HWs
            print(&#39;WARNING: more than 3 values between HWs, removing part of them&#39;)
            #print(data_pd_HWLW_1tide)
            agger35_prim = data_pd_HWLW_1tide_noHWs[data_pd_HWLW_1tide_noHWs[&#39;HWLWcode&#39;]==2]
            if len(agger35_prim)&gt;1:
                raise Exception(&#39;should be only one HWLWcode=2 per tide period&#39;)
            agger35_prim_loc = agger35_prim.index[0]
            agger35_sec_loc = data_pd_HWLW_1tide_noHWs.loc[data_pd_HWLW_1tide_noHWs[&#39;HWLWcode&#39;]==22,&#39;values&#39;].idxmin()
            agger35_loc = np.sort([agger35_prim_loc,agger35_sec_loc])
            data_pd_HWLW_1tide_noHWs = data_pd_HWLW_1tide_noHWs.loc[agger35_loc.min():agger35_loc.max(),:]
            agger4_loc = data_pd_HWLW_1tide_noHWs[&#39;values&#39;].idxmax()
            data_pd_HWLW_1tide_noHWs = data_pd_HWLW_1tide_noHWs.loc[[agger35_loc.min(),agger4_loc,agger35_loc.max()],:]
        
        if len(data_pd_HWLW_1tide_noHWs) == 1: #primary low water already has code 2
            if data_pd_HWLW_1tide_noHWs[&#39;HWLWcode&#39;].iloc[0] != 2:
                raise Exception(&#39;Only 1 LW value but does not have HWLWcode 2&#39;)
        elif len(data_pd_HWLW_1tide_noHWs) == 3:
            if not data_pd_HWLW_1tide_noHWs[&#39;values&#39;].argmax() == 1:
                raise Exception(&#39;3 values between two HW values, but center one is not the largest:\n%s&#39;%(data_pd_HWLW_1tide))
            agger345_loc = data_pd_HWLW_1tide_noHWs.index
            if not (data_pd_HWLW.loc[agger345_loc[0],&#39;HWLWcode&#39;] in [2,22] and data_pd_HWLW.loc[agger345_loc[1],&#39;HWLWcode&#39;] in [11] and data_pd_HWLW.loc[agger345_loc[2],&#39;HWLWcode&#39;] in [2,22]):
                raise Exception(&#39;3 values between two HW values, but do not correspond to LW/agger/LW:\n%s&#39;%(data_pd_HWLW_1tide))
            data_pd_HWLW.loc[agger345_loc,&#39;HWLWcode&#39;] = [3,4,5]
        elif len(data_pd_HWLW_1tide_noHWs) == 2:
            print(&#39;WARNING: 2 values left between two HWs (slightly unexpected):\n%s&#39;%(data_pd_HWLW_1tide))
        else:
            raise Exception(&#39;unexpected number of values between two HWs (0 or more than 3):\n%s&#39;%(data_pd_HWLW_1tide))
    
    #remove remaining 11 and 22 values from array
    #if calc_HWLW345_cleanup1122:
    data_pd_HWLW = data_pd_HWLW.drop(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==11].index)
    data_pd_HWLW = data_pd_HWLW.drop(data_pd_HWLW[data_pd_HWLW[&#39;HWLWcode&#39;]==22].index)
    
    print(&#39;finished calculating 1stLW/agger/2ndLW for all tidalperiods&#39;)
    
    return data_pd_HWLW</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.calc_HWLW12345to21"><code class="name flex">
<span>def <span class="ident">calc_HWLW12345to21</span></span>(<span>data_HWLW_12345)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_HWLW12345</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_HWLW12345to21(data_HWLW_12345): #TODO: if first/last timestep is LW, these are not returned (loops from HW to HW)
    &#34;&#34;&#34;
    

    Parameters
    ----------
    data_HWLW12345 : TYPE
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    print(&#39;starting HWLW 12345 to 12 correction&#39;)
    times_LWmin = []
    data_HW1 = data_HWLW_12345.loc[data_HWLW_12345[&#39;HWLWcode&#39;]==1]
    for iHW in np.arange(0,len(data_HW1)-1):
        tide_afterHW = data_HWLW_12345.loc[data_HW1.index[iHW]:data_HW1.index[iHW+1]]
        time_minimum = tide_afterHW[&#39;values&#39;].idxmin()
        times_LWmin.append(time_minimum)
    data_LW2 = data_HWLW_12345.loc[times_LWmin]
    data_LW2[&#39;HWLWcode&#39;] = 2
    data_HWLW_12 = pd.concat([data_HW1,data_LW2]).sort_index()
    
    return data_HWLW_12</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.calc_HWLWnumbering"><code class="name flex">
<span>def <span class="ident">calc_HWLWnumbering</span></span>(<span>ts_ext, station=None, corr_tideperiods=None)</span>
</code></dt>
<dd>
<div class="desc"><p>For calculation of the extremes numbering, w.r.t. the first high water at Cadzand in 2000 (occurred on 1-1-2000 at approximately 9:45).
The number of every high and low water is calculated by taking the time difference between itself and the first high water at Cadzand, correcting it with the station phase difference (M2phasediff).
Low waters are searched for half an M2 period from the high waters.
By adding a search window of half the period of M2 (searchwindow_hr), even strong time variance between consecutive high or low waters should be caputered. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts_ext</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' and 'HWLWcode' column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.</dd>
<dt><strong><code>station</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The station for which the M2 phase difference should be retrieved from data_M2phasediff_perstation.txt.
This value is the phase difference in degrees of the occurrence of the high water generated by the same tidal wave as the first high water in 2000 at Cadzand (actually difference between M2 phases of stations).
This value is used to correct the search window of high/low water numbering. The default is None.</dd>
<dt><strong><code>corr_tideperiods</code></strong> :&ensp;<code>integer</code>, optional</dt>
<dd>Test keyword to derive HWLWnumbering with a n*360 degrees offset only, but this does not work properly. The default is None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ts_ext</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The input DataFrame with the column 'HWLWno' added, which contains the numbers of the extremes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_HWLWnumbering(ts_ext, station=None, corr_tideperiods=None):
    &#34;&#34;&#34;
    For calculation of the extremes numbering, w.r.t. the first high water at Cadzand in 2000 (occurred on 1-1-2000 at approximately 9:45). 
    The number of every high and low water is calculated by taking the time difference between itself and the first high water at Cadzand, correcting it with the station phase difference (M2phasediff). 
    Low waters are searched for half an M2 period from the high waters. 
    By adding a search window of half the period of M2 (searchwindow_hr), even strong time variance between consecutive high or low waters should be caputered. 
    
    Parameters
    ----------
    ts_ext : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLWcode&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.
    station: string, optional
        The station for which the M2 phase difference should be retrieved from data_M2phasediff_perstation.txt.
        This value is the phase difference in degrees of the occurrence of the high water generated by the same tidal wave as the first high water in 2000 at Cadzand (actually difference between M2 phases of stations).
        This value is used to correct the search window of high/low water numbering. The default is None.
    corr_tideperiods : integer, optional
        Test keyword to derive HWLWnumbering with a n*360 degrees offset only, but this does not work properly. The default is None.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    ts_ext : pandas.DataFrame
        The input DataFrame with the column &#39;HWLWno&#39; added, which contains the numbers of the extremes.

    &#34;&#34;&#34;
        
    M2_period_hr = get_schureman_freqs([&#39;M2&#39;]).loc[&#39;M2&#39;,&#39;period [hr]&#39;]
    firstHWcadz_fixed = dt.datetime(2000, 1, 1, 9, 45)
    searchwindow_hr = M2_period_hr/2
    
    if not all((ts_ext[&#39;HWLWcode&#39;]==1) | (ts_ext[&#39;HWLWcode&#39;]==2) | (ts_ext[&#39;HWLWcode&#39;]==3) | (ts_ext[&#39;HWLWcode&#39;]==4) | (ts_ext[&#39;HWLWcode&#39;]==5)):
        raise Exception(&#39;calc_HWLWnumbering() not implemented for HWLWcode other than 1,2,3,4,5 (so no HWLWcode 11 or 22 supported), provide extreme timeseries derived with Timeseries.calc_HWLW(calc_HWLW345=False) or Timeseries.calc_HWLW(calc_HWLW345=True, calc_HWLW345_cleanup1122=True)&#39;)
    ts_ext = ts_ext.copy()
    
    HW_bool = ts_ext[&#39;HWLWcode&#39;]==1
    HW_tdiff_cadzdraw = (ts_ext.loc[HW_bool].index.to_series()-firstHWcadz_fixed).dt.total_seconds()/3600
    if station is None:
        HW_tdiff_cadzdraw_M2remainders = (HW_tdiff_cadzdraw)%M2_period_hr
        M2phasediff_hr = (HW_tdiff_cadzdraw_M2remainders).mean()
        M2phasediff_deg = M2phasediff_hr/M2_period_hr*360
        print(&#39;no value or None for argument M2phasediff provided, automatically calculated correction w.r.t. Cadzand is %.2f hours (%.2f degrees)&#39;%(M2phasediff_hr, M2phasediff_deg))
        if corr_tideperiods is not None:
            M2phasediff_deg = M2phasediff_deg+corr_tideperiods
            M2phasediff_hr = M2phasediff_deg/360*M2_period_hr
            print(&#39;additional tideperiod correction provided via corr_tideperiods of %.1f degrees, new correction w.r.t. Cadzand is %.2f hours (%.2f degrees)&#39;%(corr_tideperiods, M2phasediff_hr, M2phasediff_deg))
    else:
        file_M2phasediff = os.path.join(os.path.dirname(file_path),&#39;data&#39;,&#39;data_M2phasediff_perstation.txt&#39;)
        stations_M2phasediff = pd.read_csv(file_M2phasediff, names=[&#39;M2phasediff&#39;], comment=&#39;#&#39;, delim_whitespace=True)
        if station not in stations_M2phasediff.index:
            raise Exception(f&#39;ERROR: station &#34;{station}&#34; not in file_M2phasediff ({file_M2phasediff})&#39;)
        stat_M2phasediff = stations_M2phasediff.loc[station,&#39;M2phasediff&#39;]
        M2phasediff_hr = stat_M2phasediff/360*M2_period_hr
    HW_tdiff_cadzd = HW_tdiff_cadzdraw - M2phasediff_hr + searchwindow_hr
    HW_tdiff_div, HW_tdiff_mod_searchwindow = np.divmod(HW_tdiff_cadzd.values, M2_period_hr)
    HW_tdiff_mod = HW_tdiff_mod_searchwindow - searchwindow_hr
    if not all(np.diff(HW_tdiff_div) &gt; 0):
        raise Exception(&#39;tidal wave numbering: HW numbers not always increasing&#39;)
    if not all(np.abs(HW_tdiff_mod)&lt;searchwindow_hr):
        raise Exception(&#39;tidal wave numbering: not all HW fall into hardcoded search window&#39;)
    ts_ext.loc[HW_bool,&#39;HWLWno&#39;] = HW_tdiff_div
    
    for LWcode_2345 in [2,3,4,5]:
        LW_bool = ts_ext[&#39;HWLWcode&#39;]==LWcode_2345
        LW_tdiff_cadzdraw = (ts_ext.loc[LW_bool].index.to_series()-firstHWcadz_fixed).dt.total_seconds()/3600
        LW_tdiff_cadzd = LW_tdiff_cadzdraw - M2phasediff_hr + searchwindow_hr - M2_period_hr/2
        LW_tdiff_div, LW_tdiff_mod_searchwindow = np.divmod(LW_tdiff_cadzd.values, M2_period_hr)
        LW_tdiff_mod = LW_tdiff_mod_searchwindow - searchwindow_hr
        if not all(np.diff(LW_tdiff_div) &gt; 0):
            raise Exception(&#39;tidal wave numbering: LW numbers not always increasing&#39;)
        if not all(np.abs(LW_tdiff_mod)&lt;searchwindow_hr):
            raise Exception(&#39;tidal wave numbering: not all LW fall into defined search window&#39;)
        ts_ext.loc[LW_bool,&#39;HWLWno&#39;] = LW_tdiff_div
    
    #check if LW is after HW
    ts_ext_checkfirst = ts_ext[ts_ext[&#39;HWLWno&#39;]==np.min(HW_tdiff_div)]
    tdiff_firstHWLW = (ts_ext_checkfirst.index.to_series().diff().dt.total_seconds()/3600).values[1]
    if (tdiff_firstHWLW&lt;0) or (tdiff_firstHWLW&gt;M2_period_hr):
        raise Exception(&#39;tidal wave numbering: first LW does not match first HW&#39;)
    
    ts_ext[&#39;HWLWno&#39;] = ts_ext[&#39;HWLWno&#39;].astype(int)
    
    return ts_ext</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.timeseries_fft"><code class="name flex">
<span>def <span class="ident">timeseries_fft</span></span>(<span>ts_residue, min_prominence=1000, max_freqdiff=None, plot_fft=True, source='schureman')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def timeseries_fft(ts_residue, min_prominence=10**3, max_freqdiff=None, plot_fft=True, source=&#39;schureman&#39;):
    
    print(&#39;analyzing timeseries with fft and fftfreq&#39;)
    
    if ts_residue[&#39;values&#39;].isnull().sum() &gt; 0:
        raise Exception(&#39;supplied timeseries contains nan values, use pd.interpolate first (dropping them will result in non-constant timestep which is also not possible for fft)&#39;)
    y = ts_residue[&#39;values&#39;].values
    N = len(y)
    T = np.unique((ts_residue.index[1:]-ts_residue.index[:-1])).astype(float)/1e9/3600 #timestep in hours.
    if len(T)!=1:
        raise Exception(&#39;timestep of supplied timeseries should be constant for fourier analysis&#39;)
    yf = fft(y)
    power = np.abs(yf)
    freq = fftfreq(N, T[0])
    peaks, peaks_properties = ssig.find_peaks(power[freq &gt;=0], prominence=min_prominence)
    peak_freq =  freq[peaks]
    peak_power = power[peaks]
    
    if plot_fft:
        fig,ax = plt.subplots()
        ax.plot(freq[:N//2], power[:N//2])
        ax.plot(peak_freq, peak_power, &#39;ro&#39;)
        ax.grid()
        ax.set_xlim(0,0.5)
    
    if source==&#39;schureman&#39;:
        const_list_all = get_const_list_hatyan(listtype=&#39;all_schureman&#39;)
        hatyan_freqs = get_schureman_freqs(const_list=const_list_all)
    elif source==&#39;foreman&#39;:
        const_list_all = get_const_list_hatyan(listtype=&#39;all_foreman&#39;)
        dummy, hatyan_freqs = get_foreman_v0_freq(const_list=const_list_all)
        hatyan_freqs[&#39;period [hr]&#39;] = 1/hatyan_freqs[&#39;freq&#39;]
    
    const_closest = []
    for peak_freq_one in peak_freq:
        hatyan_freqs_closest = hatyan_freqs.iloc[np.argmin(np.abs(hatyan_freqs[&#39;freq&#39;]-peak_freq_one)),:] #TODO: freq is not always close enough but is still added to list
        const_closest.append(hatyan_freqs_closest.name)
    hatyan_freqs_suggestions = hatyan_freqs.loc[const_closest,[&#39;freq&#39;,&#39;period [hr]&#39;]]
    hatyan_freqs_suggestions[&#39;peak_freq&#39;] = peak_freq
    hatyan_freqs_suggestions[&#39;peak_freqdiff&#39;] = (hatyan_freqs_suggestions[&#39;freq&#39;] - hatyan_freqs_suggestions[&#39;peak_freq&#39;]).abs()
    hatyan_freqs_suggestions[&#39;peak_prominences&#39;] = peaks_properties[&#39;prominences&#39;]
    if max_freqdiff is not None:
        #select below freqdiff treshold
        hatyan_freqs_suggestions = hatyan_freqs_suggestions.loc[hatyan_freqs_suggestions[&#39;peak_freqdiff&#39;]&lt;max_freqdiff]
    print(&#39;suggested constituents+freqs from hatyan:\n%s&#39;%(hatyan_freqs_suggestions))
    
    return hatyan_freqs_suggestions</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.plot_timeseries"><code class="name flex">
<span>def <span class="ident">plot_timeseries</span></span>(<span>ts, ts_validation=None, ts_ext=None, ts_ext_validation=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a plot with the provided timeseries</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries.</dd>
<dt><strong><code>ts_validation</code></strong> :&ensp;<code>pandas.DataFrame</code>, optional</dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries. The default is None.</dd>
<dt><strong><code>ts_ext</code></strong> :&ensp;<code>pandas.DataFrame</code>, optional</dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.</dd>
<dt><strong><code>ts_ext_validation</code></strong> :&ensp;<code>pandas.DataFrame</code>, optional</dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>The generated figure handle, with which the figure can be adapted and saved.</dd>
<dt><strong><code>axs</code></strong> :&ensp;<code>(tuple of) matplotlib.axes._subplots.AxesSubplot</code></dt>
<dd>The generated axis handle, whith which the figure can be adapted.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_timeseries(ts, ts_validation=None, ts_ext=None, ts_ext_validation=None):
    &#34;&#34;&#34;
    Creates a plot with the provided timeseries

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    ts_validation : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries. The default is None.
    ts_ext : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.
    ts_ext_validation : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.

    Returns
    -------
    fig : matplotlib.figure.Figure
        The generated figure handle, with which the figure can be adapted and saved.
    axs : (tuple of) matplotlib.axes._subplots.AxesSubplot
        The generated axis handle, whith which the figure can be adapted.

    &#34;&#34;&#34;
        
    size_figure = (15,9)
    size_line_ts = 0.7
    size_marker_ts = 1
    figure_ylim_ts = [-3,3]
    figure_ylim_tsdiff = [-0.02,0.02]
    
    if ts_validation is not None:
        times_predval_ext = [min(min(ts_validation.index),min(ts.index)), max(max(ts_validation.index),max(ts.index))]

    else:
        times_predval_ext = [min(ts.index), max(ts.index)]    

    fig, (ax1, ax2) = plt.subplots(2,1,figsize=size_figure, sharex=True, gridspec_kw={&#39;height_ratios&#39;:[2,1]})
    
    ax1.set_title(&#39;hatyan timeseries&#39;)
    ax1.plot(ts.index, ts[&#39;values&#39;],&#39;o-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;ts&#39;)
    if ts_validation is not None:
        #overlap between timeseries for difference plots
        times_id_validationinpred = np.where(ts_validation.index.isin(ts.index))[0]
        times_id_predinvalidation = np.where(ts.index.isin(ts_validation.index))[0]
        ax1.plot(ts_validation.index, ts_validation[&#39;values&#39;],&#39;o-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;ts_validation&#39;, alpha=0.7)
        ax1.plot(ts.index[times_id_predinvalidation], ts[&#39;values&#39;].iloc[times_id_predinvalidation].values-ts_validation[&#39;values&#39;].iloc[times_id_validationinpred].values,&#39;go-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;difference&#39;, alpha=0.7)
    ax1.plot(times_predval_ext,[0,0],&#39;-k&#39;,linewidth=size_line_ts)
    ts_mean = np.mean(ts[&#39;values&#39;])
    ax1.plot(ts.index[[0,-1]],[ts_mean,ts_mean],&#39;-r&#39;,linewidth=size_line_ts,label=&#39;mean of ts&#39;)
    if ts_ext is not None:
        HWLW_codesnames = {1:&#39;HW (1)&#39;,
                           2:&#39;LW (2)&#39;,
                           3:&#39;LW1 (3)&#39;,
                           4:&#39;topagger (4)&#39;,
                           5:&#39;LW2 (5)&#39;,
                           11:&#39;HW_local (11)&#39;,
                           22:&#39;LW_local (22)&#39;}
        for HWLW_code in HWLW_codesnames.keys():
            iExt = ts_ext[&#39;HWLWcode&#39;]==HWLW_code
            if iExt.any():
                HWLW_name = HWLW_codesnames[HWLW_code]
                HWLW_markersize=10
                if HWLW_code in [4,11,22]:
                    HWLW_markersize=5
                ax1.plot(ts_ext.index[iExt],ts_ext[&#39;values&#39;][iExt],&#39;x&#39;,markersize=HWLW_markersize,label=HWLW_name)
    if ts_ext_validation is not None:
        vali_codes = [1,2,3,4,5]
        vali_codenames = [&#39;vali_HW&#39;,&#39;vali_LW&#39;,&#39;vali_LW1&#39;,&#39;vali_topagger&#39;,&#39;vali_LW2&#39;]
        for vali_code, vali_codename in zip(vali_codes,vali_codenames):
            vali_code_ids = ts_ext_validation[&#39;HWLWcode&#39;].values==vali_code
            if any(vali_code_ids): #only plot vali_code in legend if present in HWLW_timeseries
                ax1.plot(ts_ext_validation.index[vali_code_ids],ts_ext_validation[&#39;values&#39;][vali_code_ids],&#39;1&#39;,markersize=10,label=vali_codename)
        #print HWLW statistics
        try:
            plot_HWLW_validatestats(ts_ext=ts_ext, ts_ext_validation=ts_ext_validation, create_plot=False)        
        except:
            print(&#39;WARNING: plot_HWLW_validatestats() failed, probably due to missing HWLWno where autocalculation failed. Consider adding HWLWno to ts_ext and ts_ext_validation with calc_HWLWnumbering() before plotting.&#39;)
    ax1.set_ylim(figure_ylim_ts)
    ax2.set_xlabel(&#39;Time&#39;)
    ax1.set_ylabel(&#39;waterlevel [m]&#39;)
    ax1.legend(loc=&#39;lower right&#39;)
    ax1.grid()
    if ts_validation is not None:
        ax2.plot(ts.index[times_id_predinvalidation], ts[&#39;values&#39;].iloc[times_id_predinvalidation].values-ts_validation[&#39;values&#39;].iloc[times_id_validationinpred].values,&#39;go-&#39;,linewidth=size_line_ts,markersize=size_marker_ts, label=&#39;difference&#39;)
    ax2.plot(times_predval_ext,[0,0],&#39;-k&#39;,linewidth=size_line_ts)
    ax2.set_ylim(figure_ylim_tsdiff)
    rmse = np.nan
    if ts_validation is not None:
        overlapdiff = ts[&#39;values&#39;].iloc[times_id_predinvalidation].values-ts_validation[&#39;values&#39;].iloc[times_id_validationinpred].values
        if len(overlapdiff) != 0:
            rmse = np.sqrt(np.nanmean(overlapdiff ** 2))
    ax2.set_ylabel(&#39;timeseries difference [m], RMSE = %.5f&#39;%(rmse))
    ax2.legend(loc=&#39;lower right&#39;)
    ax2.grid()
    fig.tight_layout()
    
    axs = (ax1,ax2)
    return fig, axs</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.plot_HWLW_validatestats"><code class="name flex">
<span>def <span class="ident">plot_HWLW_validatestats</span></span>(<span>ts_ext, ts_ext_validation, create_plot=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This definition calculates (and plots and prints) some statistics when comparing extreme values.
This is done by calculating the extreme number (sort of relative to Cadzand 1jan2000, but see 'warning') and subtracting the ts_ext and ts_ext_validation dataframes based on these numbers (and HWLWcode).
It will only result in values for the overlapping extremes, other values will be NaN and are not considered for the statistics.
Warning: the calculated extreme numbers in this definition are not corrected for the real phase difference with the M2phasediff argument, the calculated extreme are fine for internal use (to match corresponding extremes) but the absolute number might be incorrect.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts_ext</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.</dd>
<dt><strong><code>ts_ext_validation</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, values and codes of the timeseries that are extremes.</dd>
<dt><strong><code>create_plot</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether to plot the time/value differences or only print the statistics. The default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>The generated figure handle, with which the figure can be adapted and saved.</dd>
<dt><strong><code>axs</code></strong> :&ensp;<code>(tuple of) matplotlib.axes._subplots.AxesSubplot</code></dt>
<dd>The generated axis handle, whith which the figure can be adapted.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_HWLW_validatestats(ts_ext, ts_ext_validation, create_plot=True):
    &#34;&#34;&#34;
    This definition calculates (and plots and prints) some statistics when comparing extreme values.
    This is done by calculating the extreme number (sort of relative to Cadzand 1jan2000, but see &#39;warning&#39;) and subtracting the ts_ext and ts_ext_validation dataframes based on these numbers (and HWLWcode).
    It will only result in values for the overlapping extremes, other values will be NaN and are not considered for the statistics.
    Warning: the calculated extreme numbers in this definition are not corrected for the real phase difference with the M2phasediff argument, the calculated extreme are fine for internal use (to match corresponding extremes) but the absolute number might be incorrect.

    Parameters
    ----------
    ts_ext : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.
    ts_ext_validation : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, values and codes of the timeseries that are extremes.
    create_plot : boolean, optional
        Whether to plot the time/value differences or only print the statistics. The default is True.

    Returns
    -------
    fig : matplotlib.figure.Figure
        The generated figure handle, with which the figure can be adapted and saved.
    axs : (tuple of) matplotlib.axes._subplots.AxesSubplot
        The generated axis handle, whith which the figure can be adapted.

    &#34;&#34;&#34;
    
    print(&#39;Calculating comparison statistics for extremes&#39;)
    if &#39;HWLWno&#39; not in ts_ext.columns or &#39;HWLWno&#39; not in ts_ext_validation.columns:
        print(&#39;HWLWno is not present in ts_ext or ts_ext_validation, trying to automatically derive it without M2phasediff argument (this might fail)&#39;)
        try:
            ts_ext_nrs = calc_HWLWnumbering(ts_ext=ts_ext)
            ts_ext_validation_nrs = calc_HWLWnumbering(ts_ext=ts_ext_validation)
        except:
            raise Exception(&#39;ERROR: deriving HWLWno failed, so HWLW statistics cannot be calculated. Add HWLWno with calc_HWLWnumbering() before calling plot_HWLW_validatestats().&#39;)
    else:
        ts_ext_nrs = ts_ext.copy()
        ts_ext_validation_nrs = ts_ext_validation.copy()
    
    #set HWLWcode and HWLWno as index, to make easy subtraction possible
    ts_ext_nrs[&#39;times&#39;] = ts_ext_nrs.index
    ts_ext_nrs = ts_ext_nrs.set_index([&#39;HWLWcode&#39;,&#39;HWLWno&#39;],drop=False)
    ts_ext_validation_nrs[&#39;times&#39;] = ts_ext_validation_nrs.index
    ts_ext_validation_nrs = ts_ext_validation_nrs.set_index([&#39;HWLWcode&#39;,&#39;HWLWno&#39;],drop=False)
    HWLW_diff = ts_ext_nrs.sub(ts_ext_validation_nrs)
    
    tdiff_minutes = HWLW_diff[&#39;times&#39;].dt.total_seconds()/60
    vdiff_cm = HWLW_diff[&#39;values&#39;]*100
    print(&#39;Time differences [minutes]&#39;)
    print(&#39;    RMSE: %.2f&#39;%(np.sqrt(np.mean(tdiff_minutes**2))))
    print(&#39;    std: %.2f&#39;%(tdiff_minutes.std()))
    print(&#39;    abs max: %.2f&#39;%(tdiff_minutes.abs().max()))
    print(&#39;    abs mean: %.2f&#39;%(tdiff_minutes.abs().mean()))
    print(&#39;    #NaN: %i of %i&#39;%(tdiff_minutes.isnull().sum(),len(vdiff_cm)))
    print(&#39;Value differences [cm]&#39;)
    print(&#39;    RMSE: %.2f&#39;%(np.sqrt(np.mean(vdiff_cm**2))))
    print(&#39;    std: %.2f&#39;%(vdiff_cm.std()))
    print(&#39;    abs max: %.2f&#39;%(vdiff_cm.abs().max()))
    print(&#39;    abs mean: %.2f&#39;%(vdiff_cm.abs().mean()))
    print(&#39;    #NaN: %i of %i&#39;%(vdiff_cm.isnull().sum(),len(vdiff_cm)))
    
    if create_plot:
        fig, ax1 = plt.subplots()
        ax1.plot(HWLW_diff.loc[1,&#39;times&#39;].dt.total_seconds()/60,HWLW_diff.loc[1,&#39;values&#39;]*100,&#39;+&#39;,label=&#39;HWdiff&#39;)
        ax1.plot(HWLW_diff.loc[2,&#39;times&#39;].dt.total_seconds()/60,HWLW_diff.loc[2,&#39;values&#39;]*100,&#39;.&#39;,label=&#39;LWdiff&#39;)
        ax1.set_xlabel(&#39;Time difference [minutes]&#39;)
        ax1.set_ylabel(&#39;Value difference [cm]&#39;)
        ax1.legend(loc=1)
        ax1.grid()
    
        axs = (ax1)
        return fig, axs</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.write_tsnetcdf"><code class="name flex">
<span>def <span class="ident">write_tsnetcdf</span></span>(<span>ts, station, vertref, filename, ts_ext=None, tzone_hr=1, nosidx=False, mode='w')</span>
</code></dt>
<dd>
<div class="desc"><p>Writes the timeseries to a netCDF file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries.</dd>
<dt><strong><code>station</code></strong> :&ensp;<code>str</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>vertref</code></strong> :&ensp;<code>str</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The filename of the netCDF file that will be written.</dd>
<dt><strong><code>ts_ext</code></strong> :&ensp;<code>pandas.DataFrame</code>, optional</dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.</dd>
<dt><strong><code>tzone_hr</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The timezone (GMT+tzone_hr) that applies to the data. The default is 1 (MET).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_tsnetcdf(ts, station, vertref, filename, ts_ext=None, tzone_hr=1, nosidx=False, mode=&#39;w&#39;):
    &#34;&#34;&#34;
    Writes the timeseries to a netCDF file

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    station : str
        DESCRIPTION.
    vertref : str
        DESCRIPTION.
    filename : str
        The filename of the netCDF file that will be written.
    ts_ext : pandas.DataFrame, optional
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes. The default is None.
    tzone_hr : int, optional
        The timezone (GMT+tzone_hr) that applies to the data. The default is 1 (MET).
    
    Returns
    -------
    None.
    
    &#34;&#34;&#34;
    
    import hatyan
    version_no = hatyan.__version__
    
    times_all = ts.index
    timeseries = ts[&#39;values&#39;]
    times_stepmin = (ts.index[1]-ts.index[0]).total_seconds()/60
    dt_analysistime = dt.datetime.now()
    data_nc = Dataset(filename, mode, format=&#34;NETCDF3_CLASSIC&#34;)
    attr_dict = {&#39;title&#39;: &#39;tidal prediction for %s to %s&#39;%(times_all[0].strftime(&#39;%Y-%m-%d %H:%M:%S&#39;), times_all[-1].strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)),
                 &#39;institution&#39;: &#39;Rijkswaterstaat&#39;,
                 &#39;source&#39;: &#39;hatyan-%s tidal analysis program of Rijkswaterstaat&#39;%(version_no),
                 &#39;timestep_min&#39;: times_stepmin}
    data_nc.setncatts(attr_dict)
    
    ncvarlist = list(data_nc.variables.keys())
    ncdimlist = list(data_nc.dimensions.keys())
    statname_len = 64
    
    if &#39;stations&#39; not in ncdimlist:
        data_nc.createDimension(&#39;stations&#39;,None)
    if &#39;statname_len&#39; not in ncdimlist:
        data_nc.createDimension(&#39;statname_len&#39;,statname_len)
    if &#39;time&#39; not in ncdimlist:
        data_nc.createDimension(&#39;time&#39;,len(times_all.tolist()))
    if &#39;analysis_time&#39; not in ncdimlist:
        data_nc.createDimension(&#39;analysis_time&#39;,1)
    
    refdate_tz = dt.datetime(1900,1,1,tzinfo=dt.timezone(dt.timedelta(hours=tzone_hr)))
    dict_statattr = {&#39;cf_role&#39;: &#39;timeseries_id&#39;}
    dict_anatimattr = {&#39;units&#39;: &#39;minutes since %s&#39;%(refdate_tz.strftime(&#39;%Y-%m-%d %H:%M:%S %z&#39;)), &#39;standard_name&#39;:&#39;forecast_reference_time&#39;, &#39;long_name&#39;:&#39;forecast_reference_time&#39;}
    dict_timattr = {&#39;units&#39;: &#39;minutes since %s&#39;%(refdate_tz.strftime(&#39;%Y-%m-%d %H:%M:%S %z&#39;))}
    dict_wlattr = {&#39;units&#39;:&#39;m&#39;, &#39;vertical_reference&#39;: vertref, &#39;standard_name&#39;: &#39;sea_surface_height_above_geopotential_datum&#39;, &#39;long_name&#39;: &#39;astronomical prediction of water level above reference level&#39;}
    dict_HWattr = {&#39;units&#39;:&#39;m&#39;, &#39;vertical_reference&#39;: vertref, &#39;standard_name&#39;: &#39;sea_surface_height_above_geopotential_datum&#39;, &#39;long_name&#39;: &#39;astronomical prediction of high water extremes above reference level&#39;}
    dict_LWattr = {&#39;units&#39;:&#39;m&#39;, &#39;vertical_reference&#39;: vertref, &#39;standard_name&#39;: &#39;sea_surface_height_above_geopotential_datum&#39;, &#39;long_name&#39;: &#39;astronomical prediction of low water extremes above reference level&#39;}
    dict_HWLWnoattr = {&#39;units&#39;:&#39;n-th tidal wave since reference wave at Cadzand on 1-1-2000&#39;} #, &#39;standard_name&#39;: &#39;&#39;, &#39;long_name&#39;: &#39;&#39;}

    if &#39;stations&#39; not in ncvarlist: #create empty variables if not yet present
        nc_newvar = data_nc.createVariable(&#39;stations&#39;,&#39;S1&#39;,(&#39;stations&#39;,&#39;statname_len&#39;,))
        nc_newvar.setncatts(dict_statattr)
    
    if &#39;analysis_time&#39; not in ncvarlist:
        nc_newvar = data_nc.createVariable(&#39;analysis_time&#39;,&#39;f8&#39;,(&#39;analysis_time&#39;,))
        nc_newvar.setncatts(dict_anatimattr)
        data_nc.variables[&#39;analysis_time&#39;][0] = date2num([dt_analysistime], units=data_nc.variables[&#39;analysis_time&#39;].units)   
    
    #current length is used as index
    nstat = data_nc.variables[&#39;stations&#39;].shape[0]
    #append current data to netcdf files
    data_nc.variables[&#39;stations&#39;][nstat,:] = stringtoarr(station, statname_len, dtype=&#39;S&#39;)
    
    #general prediction
    if &#39;time&#39; not in ncvarlist:
        nc_newvar = data_nc.createVariable(&#39;time&#39;,&#39;f8&#39;,(&#39;time&#39;,))
        nc_newvar.setncatts(dict_timattr)
        #set time contents upon creation of variable, is constant over loop
        data_nc.variables[&#39;time&#39;][:] = date2num(times_all.tolist(),units=data_nc.variables[&#39;time&#39;].units)
    if &#39;waterlevel_astro&#39; not in ncvarlist:
        nc_newvar = data_nc.createVariable(&#39;waterlevel_astro&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;time&#39;,))
        nc_newvar.setncatts(dict_wlattr)
    data_nc.variables[&#39;waterlevel_astro&#39;][nstat,:] = timeseries
    
    if ts_ext is None:
        print(&#39;no HWLW prediction written&#39;)
        data_nc.close()
        return #this skips the HWLW part of the definition
    
    #HWLW prediction
    if nosidx:
        #convert index from time to HWLWno
        data_HWLW_nosidx = ts_ext.copy()
        data_HWLW_nosidx[&#39;times&#39;] = data_HWLW_nosidx.index
        data_HWLW_nosidx = data_HWLW_nosidx.set_index(&#39;HWLWno&#39;)
        #data_HWLW_nosidx = data_HWLW_nosidx.sort_index()
        HWLWno_all = data_HWLW_nosidx.index.unique()
        #data_HW = pd.DataFrame(data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==1],index=HWLWno_all)
        #data_LW = pd.DataFrame(data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==2],index=HWLWno_all)
        data_HW = data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==1]
        data_LW = data_HWLW_nosidx.loc[data_HWLW_nosidx[&#39;HWLWcode&#39;]==2]
        bool_HW = HWLWno_all.isin(data_HW.index)
        bool_LW = HWLWno_all.isin(data_LW.index)
        
        #HWLWno
        if &#39;HWLWno&#39; not in ncdimlist:
            data_nc.createDimension(&#39;HWLWno&#39;,len(HWLWno_all))
        if &#39;HWLWno&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;HWLWno&#39;,&#39;i&#39;,(&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_HWLWnoattr)
        data_nc.variables[&#39;HWLWno&#39;][:] = HWLWno_all
        #HW
        if &#39;times_astro_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;times_astro_HW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;times_astro_HW&#39;][nstat,bool_HW] = date2num(data_HW[&#39;times&#39;].tolist(),units=data_nc.variables[&#39;times_astro_HW&#39;].units)
        if &#39;waterlevel_astro_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_HW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_HWattr)
        data_nc.variables[&#39;waterlevel_astro_HW&#39;][nstat,bool_HW] = data_HW[&#39;values&#39;]
        #LW
        if &#39;times_astro_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;times_astro_LW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,)) 
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;times_astro_LW&#39;][nstat,bool_LW] = date2num(data_LW[&#39;times&#39;].tolist(),units=data_nc.variables[&#39;times_astro_LW&#39;].units)
        if &#39;waterlevel_astro_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_LW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;HWLWno&#39;,))
            nc_newvar.setncatts(dict_LWattr)
        data_nc.variables[&#39;waterlevel_astro_LW&#39;][nstat,bool_LW] = data_LW[&#39;values&#39;]
    
    else: #use time as index and create array with gaps (not possible to combine multiple stations)
        if nstat&gt;0:
            raise Exception(f&#39;with nosidx={nosidx} it is not possible to write multiple stations per file&#39;)
        data_HWLW = ts_ext.copy()
        data_HWLW = data_HWLW.sort_index(axis=0)
        data_HW = data_HWLW[data_HWLW[&#39;HWLWcode&#39;]==1]
        data_LW = data_HWLW[data_HWLW[&#39;HWLWcode&#39;]==2]
        #create empty variables if not yet present
        
        #HW
        if &#39;time_HW&#39; not in ncdimlist:
            data_nc.createDimension(&#39;time_HW&#39;,len(data_HW))
        if &#39;time_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;time_HW&#39;,&#39;f8&#39;,(&#39;time_HW&#39;,))
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;time_HW&#39;][:] = date2num(data_HW.index.tolist(),units=data_nc.variables[&#39;time_HW&#39;].units)
        if &#39;waterlevel_astro_HW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_HW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;time_HW&#39;,))
            nc_newvar.setncatts(dict_HWattr)
        data_nc.variables[&#39;waterlevel_astro_HW&#39;][nstat,:] = data_HW[&#39;values&#39;]
        
        #LW
        if &#39;time_LW&#39; not in ncdimlist:
            data_nc.createDimension(&#39;time_LW&#39;,len(data_LW))
        if &#39;time_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;time_LW&#39;,&#39;f8&#39;,(&#39;time_LW&#39;,))
            nc_newvar.setncatts(dict_timattr)
        data_nc.variables[&#39;time_LW&#39;][:] = date2num(data_LW.index.tolist(),units=data_nc.variables[&#39;time_LW&#39;].units)
        if &#39;waterlevel_astro_LW&#39; not in ncvarlist:
            nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_LW&#39;,&#39;f8&#39;,(&#39;stations&#39;,&#39;time_LW&#39;,))
            nc_newvar.setncatts(dict_LWattr)
        data_nc.variables[&#39;waterlevel_astro_LW&#39;][nstat,:] = data_LW[&#39;values&#39;]
        
        #HWLW numbering
        if &#39;HWLWno&#39; in ts_ext.columns:
            if &#39;waterlevel_astro_HW_numbers&#39; not in ncvarlist:
                nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_HW_numbers&#39;,&#39;i4&#39;,(&#39;stations&#39;,&#39;time_HW&#39;,))
                #nc_newvar.setncatts(dict_HWattr)
            data_nc.variables[&#39;waterlevel_astro_HW_numbers&#39;][nstat,:] = data_HW[&#39;HWLWno&#39;]
            if &#39;waterlevel_astro_LW_numbers&#39; not in ncvarlist:
                nc_newvar = data_nc.createVariable(&#39;waterlevel_astro_LW_numbers&#39;,&#39;i4&#39;,(&#39;stations&#39;,&#39;time_LW&#39;,))
                #nc_newvar.setncatts(dict_LWattr)
            data_nc.variables[&#39;waterlevel_astro_LW_numbers&#39;][nstat,:] = data_LW[&#39;HWLWno&#39;]
    
    data_nc.close()
    return</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.write_tsdia"><code class="name flex">
<span>def <span class="ident">write_tsdia</span></span>(<span>ts, station, vertref, filename, headerformat='dia')</span>
</code></dt>
<dd>
<div class="desc"><p>Writes the timeseries to an equidistant dia file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries.</dd>
<dt><strong><code>station</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>vertref</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_tsdia(ts, station, vertref, filename, headerformat=&#39;dia&#39;):
    &#34;&#34;&#34;
    Writes the timeseries to an equidistant dia file

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    station : TYPE
        DESCRIPTION.
    vertref : TYPE
        DESCRIPTION.
    filename : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    
    if vertref == &#39;NAP&#39;:
        waarnemingssoort = 18
        vertreflong = &#39;T.o.v. Normaal Amsterdams Peil&#39;
    elif vertref == &#39;MSL&#39;:
        waarnemingssoort = 55
        vertreflong = &#39;T.o.v. Mean Sea Level&#39;
    else:
        raise Exception(&#39;ERROR: currently only vertref=&#34;NAP&#34; and vertref=&#34;MSL&#34; are supported for writing diafiles&#39;)
    grootheid = &#39;WATHTBRKD;Waterhoogte berekend;J&#39;
    ana = &#39;F012;Waterhoogte astronomisch mbv harmonische analyse&#39;
    
    time_today = dt.datetime.today().strftime(&#39;%Y%m%d&#39;)
    tstart_str = ts.index[0].strftime(&#39;%Y%m%d;%H%M&#39;)
    tstop_str = ts.index[-1].strftime(&#39;%Y%m%d;%H%M&#39;)
    timestep_min = (ts.index[1]-ts.index[0]).total_seconds()/60
    
    ts_values = ts[&#39;values&#39;]
    metadata_pd = pd.Series([&#39;[IDT;*DIF*;A;;%6s]&#39;%(time_today),
                             &#39;[W3H]&#39;,
                             &#39;WNS;%i&#39;%(waarnemingssoort),
                             &#39;PAR;%s&#39;%(grootheid), #parameter/grootheid, gelijk voor waarnemingssoorten 18 en 55
                             &#39;CPM;10;Oppervlaktewater&#39;, #compartiment, gelijk voor waarnemingssoorten 18 en 55
                             &#39;EHD;I;cm&#39;, #eenheid, gelijk voor waarnemingssoorten 18 en 55
                             &#39;HDH;%s;%s&#39;%(vertref,vertreflong),
                             ##&#39;ORG;NVT;Niet van toepassing&#39;,
                             ##&#39;SGK;NVT&#39;,
                             ##&#39;IVS;NVT;Niet van toepassing&#39;,
                             ##&#39;BTX;NVT;NVT;Niet van toepassing&#39;,
                             ##&#39;BTN;Niet van toepassing&#39;,
                             &#39;ANI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BHI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BMI;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;OGI;RIKZMON_WAT;RIKZ - Landelijke monitoring waterhoogten gegevens&#39;, #niet_essentieel?
                             ##&#39;GBD;NIEUWWTWG;Nieuwe Waterweg&#39;,
                             &#39;LOC;%s&#39;%(station), #;Hoek van Holland;P;RD;6793000;44400000
                             &#39;ANA;%s&#39;%(ana),
                             &#39;BEM;NVT&#39;, #niet_essentieel?
                             &#39;BEW;NVT&#39;, #niet_essentieel?
                             &#39;VAT;NVT&#39;, #niet_essentieel?
                             &#39;TYP;TE&#39;, #reekstype: equidistant
                             &#39;[RKS]&#39;,
                             &#39;TYD;%10s;%10s;%i;min&#39;%(tstart_str,tstop_str,timestep_min),
                             ##&#39;PLT;NVT;-999999999;6793000;44400000&#39;,
                             ##&#39;SYS;CENT&#39;,
                             &#39;[TPS]&#39;,
                             &#39;STA;%10s;%10s;O&#39;%(tstart_str,tstop_str),
                             &#39;[WRD]&#39;])
    if headerformat==&#39;wia&#39;:
        for metalinestart in [&#39;[IDT;&#39;,&#39;WNS&#39;]:
            bool_drop = metadata_pd.str.startswith(metalinestart)
            metadata_pd = metadata_pd[~bool_drop]
        metadata_pd[metadata_pd.str.startswith(&#39;PAR&#39;)] = &#39;GHD;%s&#39;%(grootheid)#.split(&#39;;&#39;)[0])
        metadata_pd[metadata_pd.str.startswith(&#39;CPM&#39;)] = &#39;CPM;OW;Oppervlaktewater&#39;
        metadata_pd[metadata_pd.str.startswith(&#39;ANA&#39;)] = &#39;WBM;other:%s&#39;%(ana)#.split(&#39;;&#39;)[0])
    
    linestr_list = []
    linestr = &#39;&#39;
    for iV, ts_value in enumerate(ts_values): # iterate over ts_values
        linestr_add = &#34;%i/0:&#34;%(np.round(ts_value*100))
        linestr = linestr + linestr_add
        if (len(linestr) &gt; 114) or (iV==len(ts_values)-1): # append linestr to linestr_list if linestr is longer than n characters or last item of ts_values was reached
            linestr_list.append(linestr)
            linestr = &#39;&#39;
    data_todia = pd.Series(linestr_list)
    
    with io.open(filename,&#39;w&#39;, newline=&#39;\n&#39;) as f: #open file and set linux newline style
        for metaline in metadata_pd:
            f.write(&#39;%s\n&#39;%(metaline))
        data_todia.to_csv(f,index=False,header=False)</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.write_tsdia_HWLW"><code class="name flex">
<span>def <span class="ident">write_tsdia_HWLW</span></span>(<span>ts_ext, station, vertref, filename, headerformat='dia')</span>
</code></dt>
<dd>
<div class="desc"><p>writes the extremes timeseries to a non-equidistant dia file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts_ext</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.</dd>
<dt><strong><code>station</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>vertref</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_tsdia_HWLW(ts_ext, station, vertref, filename, headerformat=&#39;dia&#39;):
    &#34;&#34;&#34;
    writes the extremes timeseries to a non-equidistant dia file

    Parameters
    ----------
    ts_ext : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the times, values and codes of the timeseries that are extremes.
    station : TYPE
        DESCRIPTION.
    vertref : TYPE
        DESCRIPTION.
    filename : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    
    if vertref == &#39;NAP&#39;:
        waarnemingssoort = 18
        vertreflong = &#39;T.o.v. Normaal Amsterdams Peil&#39;
        parameterX = &#39;GETETBRKD2;Getijextreem berekend&#39;
    elif vertref == &#39;MSL&#39;:
        waarnemingssoort = 55
        vertreflong = &#39;T.o.v. Mean Sea Level&#39;
        parameterX = &#39;GETETBRKDMSL2;Getijextreem berekend t.o.v. MSL&#39;
    else:
        raise Exception(&#39;ERROR: currently only vertref=&#34;NAP&#34; and vertref=&#34;MSL&#34; are supported for writing diafiles&#39;)
    grootheid = &#39;WATHTBRKD;Waterhoogte berekend;J&#39;
    ana = &#39;F012;Waterhoogte astronomisch mbv harmonische analyse&#39; #HW en LW uit 1 min. waterhoogten gefilterd uit 10 min. gem.
    time_today = dt.datetime.today().strftime(&#39;%Y%m%d&#39;)
    tstart_str = ts_ext.index[0].strftime(&#39;%Y%m%d;%H%M&#39;)
    tstop_str = ts_ext.index[-1].strftime(&#39;%Y%m%d;%H%M&#39;)
    
    if 11 in ts_ext[&#39;HWLWcode&#39;].values or 22 in ts_ext[&#39;HWLWcode&#39;].values:
        raise Exception(&#39;ERROR: invalid HWLWcodes in provided extreme timeseries (11 and/or 22)&#39;)
    
    metadata_pd = pd.Series([&#39;[IDT;*DIF*;A;;%6s]&#39;%(time_today),
                             &#39;[W3H]&#39;,
                             &#39;MUX;%s&#39;%(parameterX),
                             ##IVS;NVT;Niet van toepassing
                             ##BTX;NVT;NVT;Niet van toepassing
                             ##BTN;Niet van toepassing
                             &#39;ANI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BHI;RIKZITSDHG;RIKZ - afdeling ZDI te Den Haag&#39;, #niet_essentieel?
                             &#39;BMI;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;OGI;RIKZMON_WAT;RIKZ - Landelijke monitoring waterhoogten gegevens&#39;, #niet_essentieel?
                             ##GBD;NIEUWWTWG;Nieuwe Waterweg
                             &#39;LOC;%s&#39;%(station),
                             &#39;ANA;%s&#39;%(ana),
                             &#39;BEM;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;BEW;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;VAT;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;TYP;TN&#39;, #reekstype: niet-equidistant
                             &#39;[MUX]&#39;,
                             &#39;MXW;1;15&#39;,
                             &#39;MXP;1;GETETCDE;Getijextreem code;J&#39;,
                             &#39;MXC;1;10;Oppervlaktewater&#39;,
                             &#39;MXE;1;T;DIMSLS&#39;,
                             &#39;MXH;1;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;MXO;1;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;MXS;1;NVT&#39;, #niet_essentieel?
                             &#39;MXW;2;%i&#39;%(waarnemingssoort),
                             &#39;MXP;2;%s&#39;%(grootheid),
                             &#39;MXC;2;10;Oppervlaktewater&#39;,
                             &#39;MXE;2;I;cm&#39;,
                             &#39;MXH;2;%s;%s&#39;%(vertref, vertreflong),
                             &#39;MXO;2;NVT;Niet van toepassing&#39;, #niet_essentieel?
                             &#39;MXS;2;NVT&#39;, #niet_essentieel?
                             &#39;[TYP]&#39;,
                             &#39;TVL;1;1;hoogwater&#39;,
                             &#39;TVL;1;2;laagwater&#39;,
                             &#39;TVL;1;3;laagwater 1&#39;,
                             &#39;TVL;1;4;topagger&#39;,
                             &#39;TVL;1;5;laagwater 2&#39;,
                             &#39;[RKS]&#39;,
                             &#39;TYD;%10s;%10s&#39;%(tstart_str,tstop_str),
                             ##&#39;PLT;NVT;-999999999;6793000;44400000&#39;,
                             &#39;SYS;CENT&#39;, #niet_essentieel?
                             &#39;[TPS]&#39;,
                             &#39;STA;%10s;%10s;O&#39;%(tstart_str,tstop_str),
                             &#39;[WRD]&#39;])
    if headerformat==&#39;wia&#39;:
        for metalinestart in [&#39;[IDT;&#39;,&#39;MXW&#39;]:
            bool_drop = metadata_pd.str.startswith(metalinestart)
            metadata_pd = metadata_pd[~bool_drop]
        metadata_pd[metadata_pd.str.startswith(&#39;ANA&#39;)] = &#39;WBM;other:%s&#39;%(ana)#.split(&#39;;&#39;)[0])
        metadata_pd[metadata_pd.str.startswith(&#39;MXP;1&#39;)] = &#39;MXT;1;GETETTPE&#39; # GETETCDE;Getijextreem code naar GETETTPE
        metadata_pd[metadata_pd.str.startswith(&#39;MXC;1&#39;)] = &#39;MXC;1;OW;Oppervlaktewater&#39;
        metadata_pd[metadata_pd.str.startswith(&#39;MXP;2&#39;)] = &#39;MXG;2;%s&#39;%(grootheid)
        metadata_pd[metadata_pd.str.startswith(&#39;MXC;2&#39;)] = &#39;MXC;2;OW;Oppervlaktewater&#39;

    data_todia = ts_ext.index.strftime(&#39;%Y%m%d;%H%M&#39;)+&#39;;&#39;+ts_ext[&#39;HWLWcode&#39;].astype(str)+&#39;/0;&#39;+(ts_ext[&#39;values&#39;]*100).round().astype(int).astype(str)+&#39;:&#39;

    with io.open(filename,&#39;w&#39;, newline=&#39;\n&#39;) as f: #open file and set linux newline style
        for metaline in metadata_pd:
            f.write(&#39;%s\n&#39;%(metaline))
        data_todia.to_csv(f,index=False,header=False)</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.writets_noos"><code class="name flex">
<span>def <span class="ident">writets_noos</span></span>(<span>ts, filename, metadata=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes the timeseries to a noos file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries.</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def writets_noos(ts, filename, metadata=None):
    &#34;&#34;&#34;
    Writes the timeseries to a noos file

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    filename : TYPE
        DESCRIPTION.

    Returns
    -------
    None.

    &#34;&#34;&#34;
    
    timestamp = dt.datetime.now().strftime(&#39;%c&#39;)
    ts_out = pd.DataFrame({&#39;times&#39;:ts.index.strftime(&#39;%Y%m%d%H%M&#39;),&#39;values&#39;:ts[&#39;values&#39;]})
    
    if metadata is None:
        header_txt = f&#34;&#34;&#34;------------------------------------------------------
        Timeseries written by hatyan
        Created at {timestamp}
        ------------------------------------------------------
        Location    : None
        Position    : None
        Source      : None
        Unit        : waterlevel
        Analyse time: 000000000000
        Timezone    : None
        ------------------------------------------------------&#34;&#34;&#34;
    else:
        header_txt = f&#34;&#34;&#34;------------------------------------------------------\nTimeseries written by hatyan\nCreated at {timestamp}\n------------------------------------------------------\n&#34;&#34;&#34;
        for key in metadata.keys():
            header_txt = header_txt+(&#39;%-12s: %s\n&#39;%(key, metadata[key]))
        header_txt = header_txt+&#39;------------------------------------------------------&#39;
    np.savetxt(filename,ts_out,fmt=&#39;%s %7.4f&#39;,header=header_txt)</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.crop_timeseries"><code class="name flex">
<span>def <span class="ident">crop_timeseries</span></span>(<span>ts, times_ext, onlyfull=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Crops the provided timeseries</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries.</dd>
<dt><strong><code>times_ext</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ts_pd_out</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crop_timeseries(ts, times_ext, onlyfull=True):
    &#34;&#34;&#34;
    Crops the provided timeseries

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries.
    times_ext : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    ts_pd_out : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    ts_pd_in = ts
    
    print(&#39;-&#39;*50)
    print(&#39;cropping timeseries&#39;)
    if not times_ext[0]&lt;times_ext[1]:
        raise Exception(&#39;ERROR: the two times times_ext should be increasing, but they are not: %s.&#39;%(times_ext))
    if (times_ext[0] &lt; ts_pd_in.index[0]) or (times_ext[-1] &gt; ts_pd_in.index[-1]):
        message = &#39;imported timeseries is not available within entire requested period:\nrequested period:    %s to %s\nimported timeseries: %s to %s&#39;%(times_ext[0],times_ext[-1],ts_pd_in.index[0],ts_pd_in.index[-1])
        if onlyfull:
            raise Exception(&#39;ERROR: %s&#39;%(message))
        else:
            print(&#39;WARNING: %s&#39;%(message))
            
    times_selected_bool = (ts_pd_in.index &gt;= times_ext[0]) &amp; (ts_pd_in.index &lt;= times_ext[-1])
    ts_pd_out = ts_pd_in.loc[times_selected_bool]
    
    print(check_ts(ts_pd_out))
    return ts_pd_out</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.resample_timeseries"><code class="name flex">
<span>def <span class="ident">resample_timeseries</span></span>(<span>ts, timestep_min, tstart=None, tstop=None)</span>
</code></dt>
<dd>
<div class="desc"><p>resamples the provided timeseries, only overlapping timesteps are selected, so no interpolation. with tstart/tstop it is possible to extend the timeseries with NaN values.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' and 'HWLW_code' column and a pd.DatetimeIndex as index, it contains the timeseries to be resampled.</dd>
<dt><strong><code>timestep_min</code></strong> :&ensp;<code>int</code></dt>
<dd>the amount of minutes with which to resample the timeseries.</dd>
<dt><strong><code>tstart</code></strong> :&ensp;<code>dt.datetime</code>, optional</dt>
<dd>the start date for the resampled timeseries, the default is None which results in using the start date of the input ts.</dd>
<dt><strong><code>tstop</code></strong> :&ensp;<code>dt.datetime</code>, optional</dt>
<dd>the stop date for the resampled timeseries, the default is None which results in using the stop date of the input ts.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_pd_resample</code></strong> :&ensp;<code>pandas.DataFrame with a 'values' column and a pd.DatetimeIndex as index</code></dt>
<dd>the resampled timeseries.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resample_timeseries(ts, timestep_min, tstart=None, tstop=None):
    &#34;&#34;&#34;
    resamples the provided timeseries, only overlapping timesteps are selected, so no interpolation. with tstart/tstop it is possible to extend the timeseries with NaN values.

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; and &#39;HWLW_code&#39; column and a pd.DatetimeIndex as index, it contains the timeseries to be resampled.
    timestep_min : int
        the amount of minutes with which to resample the timeseries.
    tstart : dt.datetime, optional
        the start date for the resampled timeseries, the default is None which results in using the start date of the input ts.
    tstop : dt.datetime, optional
        the stop date for the resampled timeseries, the default is None which results in using the stop date of the input ts.

    Returns
    -------
    data_pd_resample : pandas.DataFrame with a &#39;values&#39; column and a pd.DatetimeIndex as index
        the resampled timeseries.

    &#34;&#34;&#34;
    
    print(&#39;-&#39;*50)
    print(&#39;resampling timeseries to %i minutes&#39;%(timestep_min))
    
    bool_duplicated_index = ts.index.duplicated()
    if bool_duplicated_index.sum()&gt;0:
        raise Exception(&#39;there are duplicated values in the ts DatetimeIndex, this is not supported by Timeseries.resample_timeseries(). Try &#34;ts_nodupl = ts[~ts.index.duplicated()]&#34;&#39;)
    
    if tstart is None:
        tstart = ts.index[0]
    if tstop is None:
        tstop = ts.index[-1]
    data_pd_resample = pd.DataFrame({},index=pd.date_range(tstart,tstop,freq=&#39;%dmin&#39;%(timestep_min))) #generate timeseries with correct tstart/tstop and interval
    data_pd_resample[&#39;values&#39;] = ts[&#39;values&#39;] #put measurements into this timeseries, matches to correct index automatically
    
    print(check_ts(data_pd_resample))
    return data_pd_resample</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.check_rayleigh"><code class="name flex">
<span>def <span class="ident">check_rayleigh</span></span>(<span>ts_pd, t_const_freq_pd)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_rayleigh(ts_pd,t_const_freq_pd):
    
    t_const_freq = t_const_freq_pd.sort_values(&#39;freq&#39;)[&#39;freq&#39;].drop(&#39;A0&#39;,errors=&#39;ignore&#39;)
    freq_diffs = np.diff(t_const_freq)
    rayleigh_tresh = 0.7 #0.99 # Koos Doekes: &#34;Bij het algoritme dat HATYAN gebruikt mag men in de praktijk het Rayleigh-criterium enigszins schenden, tot zo&#39;n 0,7 van de theoretisch vereiste reekslengte. &#34;
    rayleigh = len(ts_pd[&#39;values&#39;])*freq_diffs #TODO: might be better to drop timeseries nan-values first
    freq_diff_min = rayleigh_tresh/len(ts_pd[&#39;values&#39;])
    rayleigh_bool = rayleigh&gt;rayleigh_tresh
    rayleigh_bool_id = np.where(~rayleigh_bool)[0]
    
    if rayleigh_bool.all():
        print(&#39;Rayleigh criterion OK (always&gt;%.2f, minimum is %.2f)&#39;%(rayleigh_tresh, np.min(rayleigh)))
        print(&#39;Frequencies are far enough apart (always &gt;%.6f, minimum is %.6f)&#39;%(freq_diff_min,np.min(freq_diffs)))
    else:
        print(&#39;Rayleigh criterion vandalised (not always&gt;%.2f, minimum is %.2f)&#39;%(rayleigh_tresh, np.min(rayleigh)))
        print(&#39;Frequencies with not enough difference (not always &gt;%.6f, minimum is %.6f)&#39;%(freq_diff_min,np.min(freq_diffs)))
        for ray_id in rayleigh_bool_id:
            t_const_freq_sel = t_const_freq.iloc[[ray_id,ray_id+1]]
            t_const_freq_sel[&#39;diff&#39;] = np.diff(t_const_freq_sel.values)[0]
            print(t_const_freq_sel)
            if t_const_freq_sel[&#39;diff&#39;] &lt; 1e-9:
                print(&#39;WARNING: difference almost zero, will result in ill conditioned matrix&#39;)</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.check_ts"><code class="name flex">
<span>def <span class="ident">check_ts</span></span>(<span>ts)</span>
</code></dt>
<dd>
<div class="desc"><p>prints several statistics of the provided timeseries</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ts</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The DataFrame should contain a 'values' column and a pd.DatetimeIndex as index, it contains the timeseries to be checked.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>print_statement</code></strong> :&ensp;<code>str</code></dt>
<dd>For printing as a substring of another string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_ts(ts):
    &#34;&#34;&#34;
    prints several statistics of the provided timeseries

    Parameters
    ----------
    ts : pandas.DataFrame
        The DataFrame should contain a &#39;values&#39; column and a pd.DatetimeIndex as index, it contains the timeseries to be checked.

    Returns
    -------
    print_statement: str
        For printing as a substring of another string.

    &#34;&#34;&#34;
    
    stats = Timeseries_Statistics(ts=ts)
    return stats
    
    #TODO: THE PART BELOW IS NOT USED
    raise Exception(&#39;use hatyan.Timeseries_Statistics() instead&#39;)
    
    timesteps_min_all = ts.index.to_series().diff()[1:].dt.total_seconds()/60
    bool_int = (timesteps_min_all-timesteps_min_all.round(0))&lt;1e-9
    if bool_int.all():
        timesteps_min_all = timesteps_min_all.astype(int)
    else: #in case of non integer minute timesteps (eg seconds)
        timesteps_min_all[bool_int] = timesteps_min_all[bool_int].round(0)
    timesteps_min = set(timesteps_min_all)
    #print(timesteps_min)
    if len(timesteps_min)&lt;=100:
        timesteps_min_print = timesteps_min
    else:
        timesteps_min_print = &#39;too much unique time intervals (&gt;100) to display all of them, %i intervals ranging from %i to %i minutes&#39;%(len(timesteps_min),np.min(list(timesteps_min)),np.max(list(timesteps_min)))
    if (timesteps_min_all&gt;0).all():
        timesteps_incr_print = &#39;all time intervals are in increasing order and are never equal&#39;
    else:
        timesteps_incr_print = &#39;the times-order of ts is not always increasing (duplicate values or wrong order)&#39;
    
    ntimes_nonan = ts[&#39;values&#39;].count()
    ntimes = len(ts)
    ntimesteps_uniq = len(timesteps_min)

    
    if len(ts)==0:
        print_statement = f&#39;timeseries contents:\n{ts}&#39;
    else:
        print_statement = (f&#39;timeseries contents:\n{ts}\n&#39;+
                           f&#39;timeseries # unique timesteps: {ntimesteps_uniq}\n&#39;+
                           f&#39;timeseries unique timesteps (minutes):\n{timesteps_min_print}\n&#39;+
                           f&#39;timeseries validity: {timesteps_incr_print}\n&#39;+
                           f&#39;timeseries length: {ntimes}\n&#39;+
                           f&#39;timeseries # nonan: {ntimes_nonan}\n&#39;+
                           f&#39;timeseries % nonan: {(ntimes_nonan/ntimes*100):.1f}%\n&#39;+
                           f&#39;timeseries # nan: {ntimes-ntimes_nonan}\n&#39;+
                           f&#39;timeseries % nan: {(ntimes-ntimes_nonan)/ntimes*100:.1f}%&#39;)
    return print_statement</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.get_diablocks_startstopstation"><code class="name flex">
<span>def <span class="ident">get_diablocks_startstopstation</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets information about the data blocks present in a dia file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>diablocks_pd_startstopstation</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Pandas DataFrame with 'block_starts','data_starts','data_ends','station'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_diablocks_startstopstation(filename):
    &#34;&#34;&#34;
    Gets information about the data blocks present in a dia file

    Parameters
    ----------
    filename : TYPE
        DESCRIPTION.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    diablocks_pd_startstopstation: pd.DataFrame
        Pandas DataFrame with &#39;block_starts&#39;,&#39;data_starts&#39;,&#39;data_ends&#39;,&#39;station&#39;

    &#34;&#34;&#34;
    
    #get list of starts/ends of datasets in diafile
    linenum_colnames = [&#39;block_starts&#39;,&#39;data_starts&#39;,&#39;data_ends&#39;]
    diablocks_pd_startstopstation = pd.DataFrame({},columns=linenum_colnames)
    
    with open(filename, encoding=&#39;latin1&#39;) as f: #&#39;latin1 is nodig om predictie diafile die rechtstreeks uit hatyan komen in te lezen (validatietijdserie met op regel 4 (PAR) ongeldige tekens aan het einde)
        block_id = -1
        for linenum, line in enumerate(f, 1):
            if &#39;[W3H]&#39; in line:
                block_id += 1
                diablocks_pd_startstopstation.loc[block_id,&#39;block_starts&#39;] = linenum
            elif &#39;[WRD]&#39; in line:
                diablocks_pd_startstopstation.loc[block_id,&#39;data_starts&#39;] = linenum
            elif &#39;LOC&#39; in line:
                diablocks_pd_startstopstation.loc[block_id,&#39;station&#39;] = line.rstrip().split(&#39;;&#39;)[1]
    diablocks_pd_startstopstation[&#39;data_ends&#39;] = (diablocks_pd_startstopstation[&#39;block_starts&#39;]-1).tolist()[1:]+[linenum]
    if diablocks_pd_startstopstation.isnull().any().any():
        raise Exception(&#39;ERROR: multiple blocks in diafile, but unequal amount of start/end/datastart/stationnames&#39;)
    
    #convert columns with line numbers to integers
    diablocks_pd_startstopstation[linenum_colnames] = diablocks_pd_startstopstation[linenum_colnames].astype(int)
    
    return diablocks_pd_startstopstation</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.get_diablocks"><code class="name flex">
<span>def <span class="ident">get_diablocks</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_diablocks(filename):
    
    print(&#39;reading file: %s&#39;%(filename))
    diablocks_pd = get_diablocks_startstopstation(filename)
    for block_id in diablocks_pd.index.tolist():
        #read diafile metadata as pandas series, prevent splitting of combined paramater names like MXH;2 by replacing ; with !
        data_meta_nrows = diablocks_pd.loc[block_id,&#39;data_starts&#39;] - diablocks_pd.loc[block_id,&#39;block_starts&#39;]
        data_meta_series = pd.read_table(filename,skiprows=diablocks_pd.loc[block_id,&#39;block_starts&#39;],nrows=data_meta_nrows,header=None)[0] #series of metadata
        if not data_meta_series.str.contains(&#39;GHD|MXG;2&#39;).any(): #wia files contain these parameters, dia files don&#39;t. Replace dia names with wia names (wia files also contain PAR and MXP;2, but they should not be replaced)
            data_meta_series = data_meta_series.str.replace(&#39;PAR&#39;,&#39;GHD&#39;).str.replace(&#39;MXP;2&#39;,&#39;MXG;2&#39;)
        bool_combinedparname = (data_meta_series.str[3:6]==&#39;;1;&#39;) | (data_meta_series.str[3:6]==&#39;;2;&#39;)
        data_meta_series.loc[bool_combinedparname] = data_meta_series.loc[bool_combinedparname].str.slice_replace(3,4,&#39;!&#39;)
        
        #get groepering and whether dia/wia is equidistant or non-equidistant
        bool_startswithmux = data_meta_series.str.startswith(&#39;MUX&#39;)
        row_TYP = data_meta_series.loc[data_meta_series.str.startswith(&#39;TYP&#39;)].iloc[0].split(&#39;;&#39;)[1]
        diablocks_pd.loc[block_id,&#39;TYP&#39;] = row_TYP
        if row_TYP==&#39;TN&#39;: #bool_startswithmux.any(): #extreme waterlevel timeseries (non-equidistant)
            mincontent = [&#39;MXG;2&#39;,&#39;LOC&#39;,&#39;MXH;2&#39;,&#39;MXE;2&#39;,&#39;TYD&#39;,&#39;STA&#39;]
            diablocks_pd.loc[block_id,&#39;groepering&#39;] = data_meta_series.loc[bool_startswithmux].iloc[0].split(&#39;;&#39;)[1]
        elif row_TYP==&#39;TE&#39;: #normal waterlevel timeseries (equidistant)
            mincontent = [&#39;GHD&#39;,  &#39;LOC&#39;,&#39;HDH&#39;,  &#39;EHD&#39;,  &#39;TYD&#39;,&#39;STA&#39;] #WNS,CPM,HDH,ANA
            diablocks_pd.loc[block_id,&#39;groepering&#39;] = &#39;NVT&#39;
        else:
            raise Exception(f&#39;TYP &#34;{row_TYP}&#34; not implemented in hatyan.readts_dia()&#39;)
        
        #read all required metadata
        for get_content_sel in mincontent:
            bool_mincontent = data_meta_series.str.replace(&#39;!&#39;,&#39;;&#39;).str.startswith(get_content_sel)
            if bool_mincontent.sum()!=1:
                if get_content_sel!=&#39;STA&#39;:
                    raise Exception(f&#39;unexpected amount of matched metadatalines ({bool_mincontent.sum()}) for {get_content_sel}&#39;)
            data_meta_mincontent = data_meta_series.loc[bool_mincontent].iloc[0].split(&#39;;&#39;) #list type
            if get_content_sel in [&#39;GHD&#39;,&#39;MXG;2&#39;]: # Grootheid (dia/wia, dia/wia equidistant). Originally dia contains PAR and MXP;2, but they are replaced
                file_grootheidname = data_meta_mincontent[1]
                valid_grootheidnames = [&#39;WATHTE&#39;,&#39;WATHTBRKD&#39;,&#39;NVT&#39;] #NVT in wia files
                if file_grootheidname not in valid_grootheidnames:
                    raise Exception(&#39;ERROR: grootheid name (%s) should be in %s but is %s&#39;%(get_content_sel, valid_grootheidnames, file_grootheidname))
                diablocks_pd.loc[block_id,&#39;grootheid&#39;] = file_grootheidname
            elif get_content_sel in [&#39;LOC&#39;]: # Locatie. same in all files
                coords_pd = pd.DataFrame({&#39;epsg_in&#39;:[28992,4326,4230], &#39;factor&#39;:[100,1000000,1000000]}, index=[&#39;RD&#39;,&#39;W84&#39;,&#39;E50&#39;])
                if len(data_meta_mincontent)&lt;7:
                    print(&#39;no coordinate data available in LOC line of dia file&#39;)
                    continue
                coordsys_str, coord_x, coord_y = data_meta_mincontent[4:]
                if coordsys_str not in coords_pd.index:
                    raise Exception(&#39;unknown coordinate system string in diafile ({coordsys_str})&#39;)
                diablocks_pd.loc[block_id,&#39;x&#39;] = int(coord_x)/coords_pd.loc[coordsys_str,&#39;factor&#39;]
                diablocks_pd.loc[block_id,&#39;y&#39;] = int(coord_y)/coords_pd.loc[coordsys_str,&#39;factor&#39;]
                diablocks_pd.loc[block_id,&#39;coordsys&#39;] = coordsys_str
                diablocks_pd.loc[block_id,&#39;epsg&#39;] = coords_pd.loc[coordsys_str,&#39;epsg_in&#39;]
            elif get_content_sel in [&#39;EHD&#39;,&#39;MXE;2&#39;]: # Eenheid. equidistant dia/wia, non-equidistant dia/wia
                file_eenheid = data_meta_mincontent[2]
                if file_eenheid != &#39;cm&#39;:
                    raise Exception(&#39;unknown eenheid in diafile: %s&#39;%(file_eenheid))
                diablocks_pd.loc[block_id,&#39;eenheid&#39;] = file_eenheid
            elif get_content_sel in [&#39;HDH&#39;,&#39;MXH;2&#39;]: # Hoedanigheid (NAP/MSL). equidistant dia/wia, non-equidistant dia/wia
                diablocks_pd.loc[block_id,&#39;vertref&#39;] = data_meta_mincontent[1]
            elif get_content_sel in [&#39;TYD&#39;]: #Tijdstip. same in all files
                datestart = dt.datetime.strptime(data_meta_mincontent[1]+data_meta_mincontent[2], &#34;%Y%m%d%H%M&#34;)
                datestop = dt.datetime.strptime(data_meta_mincontent[3]+data_meta_mincontent[4], &#34;%Y%m%d%H%M&#34;)
                if len(data_meta_mincontent)==5: #nonequidistant timeseries
                    timestep_value = None
                elif len(data_meta_mincontent)==7: #equidistant timeseries contains also timeunit and timestep
                    timestep_unit = data_meta_mincontent[6]
                    if timestep_unit != &#39;min&#39;:
                        raise Exception(&#39;ERROR: time unit from TYD is in unknown format (not &#34;min&#34;)&#39;)
                    timestep_value = int(data_meta_mincontent[5]) #int(timestep_value_raw)
                else:
                    raise Exception(f&#39;ERROR: time metadata is not understood: {data_meta_mincontent}&#39;)
                diablocks_pd.loc[block_id,&#39;tstart&#39;] = datestart
                diablocks_pd.loc[block_id,&#39;tstop&#39;] = datestop
                diablocks_pd.loc[block_id,&#39;timestep_min&#39;] = timestep_value
            elif get_content_sel in [&#39;STA&#39;]: #Status. same in all files
                diablocks_pd.loc[block_id,&#39;STA&#39;] = &#39;!&#39;.join(data_meta_series.loc[bool_mincontent].tolist())
    return diablocks_pd</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.readts_dia_nonequidistant"><code class="name flex">
<span>def <span class="ident">readts_dia_nonequidistant</span></span>(<span>filename, diablocks_pd, block_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def readts_dia_nonequidistant(filename, diablocks_pd, block_id):

    data_nrows = diablocks_pd.loc[block_id,&#39;data_ends&#39;] - diablocks_pd.loc[block_id,&#39;data_starts&#39;]
    data_pd_HWLW = pd.read_csv(filename,skiprows=diablocks_pd.loc[block_id,&#39;data_starts&#39;],nrows=data_nrows, header=None, names=[&#39;date&#39;,&#39;time&#39;,&#39;HWLWcode/qualitycode&#39;,&#39;valuecm:&#39;], sep=&#39;;&#39;, parse_dates={&#39;times&#39;:[0,1]})
    
    #convert HWLW+quality code to separate columns
    data_pd_HWLWtemp = data_pd_HWLW.loc[:,&#39;HWLWcode/qualitycode&#39;].str.split(&#39;/&#39;, expand=True)
    data_pd_HWLW[&#39;HWLWcode&#39;] = data_pd_HWLWtemp.iloc[:,0].astype(&#39;int&#39;)
    data_pd_HWLW[&#39;qualitycode&#39;] = data_pd_HWLWtemp.iloc[:,1].astype(&#39;int&#39;)
    data_pd_HWLW = data_pd_HWLW.drop(&#39;HWLWcode/qualitycode&#39;,axis=&#39;columns&#39;)

    #convert value from cm to m
    data_pd_HWLW[&#39;values&#39;] = data_pd_HWLW[&#39;valuecm:&#39;].str.strip(&#39;:&#39;).astype(&#39;int&#39;)/100
    data_pd_HWLW = data_pd_HWLW.drop(&#39;valuecm:&#39;,axis=&#39;columns&#39;)
    
    bool_hiaat = data_pd_HWLW[&#39;qualitycode&#39;] == 99
    data_pd_HWLW.loc[bool_hiaat,&#39;values&#39;] = np.nan
    
    data_pd = data_pd_HWLW
    data_pd = data_pd.set_index(&#39;times&#39;)
    
    return data_pd</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.readts_dia_equidistant"><code class="name flex">
<span>def <span class="ident">readts_dia_equidistant</span></span>(<span>filename, diablocks_pd, block_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def readts_dia_equidistant(filename, diablocks_pd, block_id):
    
    datestart = diablocks_pd.loc[block_id,&#39;tstart&#39;]
    datestop = diablocks_pd.loc[block_id,&#39;tstop&#39;]
    timestep_min = diablocks_pd.loc[block_id,&#39;timestep_min&#39;]
    times_fromfile = pd.date_range(start=datestart,end=datestop,freq=&#39;%dmin&#39;%(timestep_min))
    
    #get data for station
    data_nrows = diablocks_pd.loc[block_id,&#39;data_ends&#39;] - diablocks_pd.loc[block_id,&#39;data_starts&#39;]
    data_pd = pd.read_csv(filename,skiprows=diablocks_pd.loc[block_id,&#39;data_starts&#39;],nrows=data_nrows, header=None)
    data_pdser = data_pd[0].str.strip()
    data = data_pdser.str.cat()
    data = data.strip(&#39;:&#39;) #remove first and/or last colon if present
    data = data.split(&#39;:&#39;)
    
    if len(times_fromfile) != len(data):
        raise Exception(&#39;ERROR: times and values ts are not of equal length\nlen(times_fromfile): %d\nlen(data): %d&#39;%(len(times_fromfile),len(data)))
    data_pd = pd.DataFrame({&#39;times&#39;:times_fromfile,&#39;valuecm/qualitycode&#39;:data})
    
    #convert HWLW+quality code to separate columns
    data_pd_temp = data_pd.loc[:,&#39;valuecm/qualitycode&#39;].str.split(&#39;/&#39;, expand=True)
    data_pd[&#39;values&#39;] = data_pd_temp.iloc[:,0].astype(&#39;int&#39;)/100
    data_pd[&#39;qualitycode&#39;] = data_pd_temp.iloc[:,1].astype(&#39;int&#39;)
    data_pd = data_pd.drop(&#39;valuecm/qualitycode&#39;,axis=&#39;columns&#39;)

    bool_hiaat = data_pd[&#39;qualitycode&#39;] == 99
    data_pd.loc[bool_hiaat,&#39;values&#39;] = np.nan
    data_pd = data_pd.set_index(&#39;times&#39;)
    
    return data_pd</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.readts_dia"><code class="name flex">
<span>def <span class="ident">readts_dia</span></span>(<span>filename, station=None, block_ids=None, get_status=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads an equidistant or non-equidistant dia file, or a list of dia files. Also works for diafiles containing multiple blocks for one station.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>station</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION. The default is None.</dd>
<dt><strong><code>block_ids</code></strong> :&ensp;<code>int, list</code> of <code>int</code> or <code>'allstation'</code>, optional</dt>
<dd>DESCRIPTION. The default is None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_pd</code></strong> :&ensp;<code>pandas.core.frame.DataFrame</code></dt>
<dd>DataFrame with a 'values' column and a pd.DatetimeIndex as index in case of an equidistant file, or more columns in case of a non-equidistant file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def readts_dia(filename, station=None, block_ids=None, get_status=False):
    &#34;&#34;&#34;
    Reads an equidistant or non-equidistant dia file, or a list of dia files. Also works for diafiles containing multiple blocks for one station.

    Parameters
    ----------
    filename : TYPE
        DESCRIPTION.
    station : TYPE
        DESCRIPTION. The default is None.
    block_ids : int, list of int or &#39;allstation&#39;, optional
        DESCRIPTION. The default is None.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    data_pd : pandas.core.frame.DataFrame
        DataFrame with a &#39;values&#39; column and a pd.DatetimeIndex as index in case of an equidistant file, or more columns in case of a non-equidistant file.

    &#34;&#34;&#34;
    
    if not isinstance(filename,list):
        filename = [filename]
    if len(filename)==0:
        raise Exception(&#39;ERROR: filename list is empty&#39;)
    
    data_pd_all = pd.DataFrame()
    for iF, filename_one in enumerate(filename):    
        diablocks_pd = get_diablocks(filename_one)
        pd.set_option(&#39;display.max_columns&#39;, 6) #default was 0, but need more to display groepering
        pd.set_option(&#39;display.width&#39;, 200) #default was 80, but need more to display groepering
        print_cols = [&#39;block_starts&#39;, &#39;station&#39;, &#39;grootheid&#39;, &#39;groepering&#39;, &#39;tstart&#39;, &#39;tstop&#39;]
        print(&#39;blocks in diafile:\n%s&#39;%(diablocks_pd[print_cols]))
        str_getdiablockspd = &#39;A summary of the available blocks is printed above, obtain a full DataFrame of available diablocks with &#34;diablocks_pd=Timeseries.get_diablocks(filename)&#34;&#39;
        
        #get equidistant timeseries from metadata
        if block_ids is None or block_ids==&#39;allstation&#39;:
            if station is None:
                raise Exception(&#39;ERROR: if block_ids argument is not provided (or None) or is &#34;allstation&#34;, station argument should be provided.&#39;)
            bool_station = diablocks_pd[&#39;station&#39;]==station
            ids_station = diablocks_pd[bool_station].index.tolist()
            if len(ids_station)&lt;1:
                raise Exception(&#39;ERROR: no data block with requested station (%s) present in dia file. %s&#39;%(station, str_getdiablockspd))
            elif len(ids_station)&gt;1 and block_ids is None:
                raise Exception(&#39;ERROR: more than one data block with requested station (%s) present in dia file. Provide block_ids argument to readts_dia() (int, list of int or &#34;allstation&#34;). %s&#39;%(station, str_getdiablockspd))
            else: #exactly one occurrence or block_ids is provided or block_ids=&#39;allstation&#39;
                block_ids = ids_station
        
        #check validity of blockids of type listlist
        if isinstance(block_ids,int):
            block_ids = [block_ids]
        if not isinstance(block_ids,list):
            raise Exception(&#39;ERROR: invalid type for block_ids (should be int, list of int or &#34;allstation&#34;)&#39;)
        if not pd.Series(block_ids).isin(diablocks_pd.index).all():
            raise Exception(f&#39;ERROR: invalid values in block_ids list ({block_ids}), possible are {diablocks_pd.index.tolist()} (all integers)&#39;)
            
        if station is not None:
            if not isinstance(station,str):
                raise Exception(&#39;ERROR: station argument should be of type string&#39;)
            bool_samestation = diablocks_pd.loc[block_ids,&#39;station&#39;]==station
            if not bool_samestation.all():
                raise Exception(&#39;ERROR: both the arguments station and block_ids are provided, but at least one of the requested block_ids corresponds to a different station. %s&#39;%(str_getdiablockspd))
            
        data_pd_allblocks = pd.DataFrame()
        for block_id in block_ids:
            if np.isnan(diablocks_pd.loc[block_id,&#39;timestep_min&#39;]): #non-equidistant
                data_pd_oneblock = readts_dia_nonequidistant(filename_one, diablocks_pd, block_id)
            else: #equidistant
                data_pd_oneblock = readts_dia_equidistant(filename_one, diablocks_pd, block_id)
            if get_status: #TODO: this can be more generic (eg add additional metadata) or more neat. Also in get_diablocks()
                block_status_list = diablocks_pd.loc[block_id,&#39;STA&#39;].split(&#39;!&#39;)
                for block_status_one in block_status_list:
                    status_tstart = dt.datetime.strptime(block_status_one[4:17],&#39;%Y%m%d;%H%M&#39;)
                    status_tstop = dt.datetime.strptime(block_status_one[18:31],&#39;%Y%m%d;%H%M&#39;)
                    status_val = block_status_one[-1]
                    data_pd_oneblock.loc[status_tstart:status_tstop,&#39;Status&#39;] = status_val
            data_pd_allblocks = data_pd_allblocks.append(data_pd_oneblock, ignore_index=False)
        
        #append to allyears dataset
        data_pd_all = data_pd_all.append(data_pd_allblocks, ignore_index=False)

    #check overlapping timesteps, sort values on time and check_ts
    if len(data_pd_all) != len(data_pd_all.index.unique()):
        raise Exception(&#39;ERROR: merged datasets have duplicate/overlapping timesteps, clean up your input data or provide one file instead of a list&#39;)
    data_pd_all = data_pd_all.sort_index(axis=0)
    print(check_ts(data_pd_all))
    
    return data_pd_all</code></pre>
</details>
</dd>
<dt id="hatyan.timeseries.readts_noos"><code class="name flex">
<span>def <span class="ident">readts_noos</span></span>(<span>filename, datetime_format='%Y%m%d%H%M', na_values=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a noos file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>datetime_format</code></strong> :&ensp;<code>TYPE</code>, optional</dt>
<dd>DESCRIPTION. The default is '%Y%m%d%H%M'.</dd>
<dt><strong><code>na_values</code></strong> :&ensp;<code>TYPE</code>, optional</dt>
<dd>DESCRIPTION. The default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_pd</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def readts_noos(filename, datetime_format=&#39;%Y%m%d%H%M&#39;, na_values=None):
    &#34;&#34;&#34;
    Reads a noos file

    Parameters
    ----------
    filename : TYPE
        DESCRIPTION.
    datetime_format : TYPE, optional
        DESCRIPTION. The default is &#39;%Y%m%d%H%M&#39;.
    na_values : TYPE, optional
        DESCRIPTION. The default is None.

    Returns
    -------
    data_pd : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    
    print(&#39;-&#39;*50)
    print(&#39;reading file: %s&#39;%(filename))
    noosheader = []
    noosheader_dict = {}
    with open(filename) as f:
        for linenum, line in enumerate(f, 0):
            if &#39;#&#39; in line:
                noosheader.append(line)
                comment_stripped = line.strip(&#39;#&#39;).strip().split(&#39;: &#39;)
                if len(comment_stripped) == 1:
                    if comment_stripped[0] != &#39;&#39;:
                        noosheader_dict[comment_stripped[0]] = &#39;&#39;
                else:
                    noosheader_dict[comment_stripped[0].strip()] = comment_stripped[1].strip()
            else:
                startdata = linenum
                break
    
    content_pd = pd.read_csv(filename,header=startdata-1,delim_whitespace=True,names=[&#39;times_str&#39;,&#39;values&#39;], na_values=na_values)
    noos_datetime = pd.to_datetime(content_pd[&#39;times_str&#39;],format=datetime_format)
    data_pd = pd.DataFrame({&#39;values&#39;:content_pd[&#39;values&#39;].values},index=noos_datetime)
    
    print(check_ts(data_pd))
    return data_pd</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hatyan.timeseries.Timeseries_Statistics"><code class="flex name class">
<span>class <span class="ident">Timeseries_Statistics</span></span>
<span>(</span><span>ts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Timeseries_Statistics:
    #TODO: make like a dict with different __str__ method, instead of this mess https://stackoverflow.com/questions/4014621/a-python-class-that-acts-like-dict
    #TODO: improve output dict, keys are now not convenient to use. Maybe make keys and longname?
    def __init__(self,ts):
        timesteps_min_all = ts.index.to_series().diff()[1:].dt.total_seconds()/60
        bool_int = (timesteps_min_all-timesteps_min_all.round(0))&lt;1e-9
        if bool_int.all():
            timesteps_min_all = timesteps_min_all.astype(int)
        else: #in case of non integer minute timesteps (eg seconds)
            timesteps_min_all[bool_int] = timesteps_min_all[bool_int].round(0)
        timesteps_min = set(timesteps_min_all)
        #print(timesteps_min)
        if len(timesteps_min)&lt;=100:
            timesteps_min_print = timesteps_min
        else:
            timesteps_min_print = &#39;too much unique time intervals (&gt;100) to display all of them, %i intervals ranging from %i to %i minutes&#39;%(len(timesteps_min),np.min(list(timesteps_min)),np.max(list(timesteps_min)))
        if (timesteps_min_all&gt;0).all():
            timesteps_incr_print = &#39;all time intervals are in increasing order and are never equal&#39;
        else:
            timesteps_incr_print = &#39;the times-order of ts is not always increasing (duplicate values or wrong order)&#39;
        
        ntimes_nonan = ts[&#39;values&#39;].count()
        ntimes = len(ts)
        ntimesteps_uniq = len(timesteps_min)
        if len(ts)==0:
            self.stats = {&#39;timeseries contents&#39;:ts}
        else:
            self.stats = {&#39;timeseries contents&#39;:ts,
                        &#39;timeseries # unique timesteps&#39;: ntimesteps_uniq,
                        &#39;timeseries unique timesteps (minutes)&#39;:timesteps_min_print,
                        &#39;timeseries validity&#39;: timesteps_incr_print,
                        &#39;timeseries length&#39;: ntimes,
                        &#39;timeseries # nonan&#39;: ntimes_nonan,
                        &#39;timeseries % nonan&#39;: ntimes_nonan/ntimes*100,#%.1f %
                        &#39;timeseries # nan&#39;: ntimes-ntimes_nonan,
                        &#39;timeseries % nan&#39;: (ntimes-ntimes_nonan)/ntimes*100, #%.1f %
                        }
    def __str__(self):
        print_statement = &#39;&#39;
        for key in self.stats.keys():
            if key in [&#39;timeseries contents&#39;,&#39;timeseries unique timesteps (minutes)&#39;]:
                print_statement += f&#39;{key}:\n{self.stats[key]}\n&#39;
            else:
                print_statement += f&#39;{key}: {self.stats[key]}\n&#39;
        return print_statement
    def __repr__(self): #avoid printing the class name
        #return dict.__repr__
        return str(self.stats)
    &#34;&#34;&#34;
    @classmethod
    def keys(self):
        return self.stats.keys()
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hatyan" href="index.html">hatyan</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hatyan.timeseries.calc_HWLW" href="#hatyan.timeseries.calc_HWLW">calc_HWLW</a></code></li>
<li><code><a title="hatyan.timeseries.calc_HWLWlocalto345" href="#hatyan.timeseries.calc_HWLWlocalto345">calc_HWLWlocalto345</a></code></li>
<li><code><a title="hatyan.timeseries.calc_HWLW12345to21" href="#hatyan.timeseries.calc_HWLW12345to21">calc_HWLW12345to21</a></code></li>
<li><code><a title="hatyan.timeseries.calc_HWLWnumbering" href="#hatyan.timeseries.calc_HWLWnumbering">calc_HWLWnumbering</a></code></li>
<li><code><a title="hatyan.timeseries.timeseries_fft" href="#hatyan.timeseries.timeseries_fft">timeseries_fft</a></code></li>
<li><code><a title="hatyan.timeseries.plot_timeseries" href="#hatyan.timeseries.plot_timeseries">plot_timeseries</a></code></li>
<li><code><a title="hatyan.timeseries.plot_HWLW_validatestats" href="#hatyan.timeseries.plot_HWLW_validatestats">plot_HWLW_validatestats</a></code></li>
<li><code><a title="hatyan.timeseries.write_tsnetcdf" href="#hatyan.timeseries.write_tsnetcdf">write_tsnetcdf</a></code></li>
<li><code><a title="hatyan.timeseries.write_tsdia" href="#hatyan.timeseries.write_tsdia">write_tsdia</a></code></li>
<li><code><a title="hatyan.timeseries.write_tsdia_HWLW" href="#hatyan.timeseries.write_tsdia_HWLW">write_tsdia_HWLW</a></code></li>
<li><code><a title="hatyan.timeseries.writets_noos" href="#hatyan.timeseries.writets_noos">writets_noos</a></code></li>
<li><code><a title="hatyan.timeseries.crop_timeseries" href="#hatyan.timeseries.crop_timeseries">crop_timeseries</a></code></li>
<li><code><a title="hatyan.timeseries.resample_timeseries" href="#hatyan.timeseries.resample_timeseries">resample_timeseries</a></code></li>
<li><code><a title="hatyan.timeseries.check_rayleigh" href="#hatyan.timeseries.check_rayleigh">check_rayleigh</a></code></li>
<li><code><a title="hatyan.timeseries.check_ts" href="#hatyan.timeseries.check_ts">check_ts</a></code></li>
<li><code><a title="hatyan.timeseries.get_diablocks_startstopstation" href="#hatyan.timeseries.get_diablocks_startstopstation">get_diablocks_startstopstation</a></code></li>
<li><code><a title="hatyan.timeseries.get_diablocks" href="#hatyan.timeseries.get_diablocks">get_diablocks</a></code></li>
<li><code><a title="hatyan.timeseries.readts_dia_nonequidistant" href="#hatyan.timeseries.readts_dia_nonequidistant">readts_dia_nonequidistant</a></code></li>
<li><code><a title="hatyan.timeseries.readts_dia_equidistant" href="#hatyan.timeseries.readts_dia_equidistant">readts_dia_equidistant</a></code></li>
<li><code><a title="hatyan.timeseries.readts_dia" href="#hatyan.timeseries.readts_dia">readts_dia</a></code></li>
<li><code><a title="hatyan.timeseries.readts_noos" href="#hatyan.timeseries.readts_noos">readts_noos</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hatyan.timeseries.Timeseries_Statistics" href="#hatyan.timeseries.Timeseries_Statistics">Timeseries_Statistics</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>